{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.utils import (\n",
    "    get_all_es_index_combinations,\n",
    "    get_all_openai_model_combinations,\n",
    "    get_all_qdrant_model_combinations,\n",
    ")\n",
    "\n",
    "\n",
    "es_index_combinations = get_all_es_index_combinations()\n",
    "qdrant_model_combinations = get_all_qdrant_model_combinations()\n",
    "openai_model_combinations = get_all_openai_model_combinations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from qdrant_client import QdrantClient\n",
    "from cache.cache import Cache\n",
    "\n",
    "\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "es_client = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"],\n",
    ")\n",
    "cache = Cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.names import DATASET_SEED\n",
    "from dataset.polqa_dataset_getter import PolqaDatasetGetter\n",
    "from dataset.poquad_dataset_getter import PoquadDatasetGetter\n",
    "from evaluation.retriever_evaluator import RetrieverEvaluator\n",
    "\n",
    "poquad_dataset_getter = PoquadDatasetGetter()\n",
    "polqa_dataset_getter = PolqaDatasetGetter()\n",
    "\n",
    "poquad_dataset = poquad_dataset_getter.get_random_n_test(500, DATASET_SEED)\n",
    "polqa_dataset = polqa_dataset_getter.get_random_n_test(500, DATASET_SEED)\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict\n",
    "\n",
    "from common.dataset_entry import DatasetEntry\n",
    "from repository.repository import Repository\n",
    "from retrievers.retriever import Retriever\n",
    "\n",
    "def run_poquad_evaluation(dataset: list[DatasetEntry], repository: Repository, retriever: Retriever, dataset_key: str):\n",
    "    scores: Dict[str, float] = {}\n",
    "    \n",
    "    ndcgs = []\n",
    "    mrrs = []\n",
    "    recalls = []\n",
    "    accuracies = []\n",
    "\n",
    "    for entry in dataset:\n",
    "        passage_id = entry.passage_id\n",
    "        query = entry.question\n",
    "        result = retriever.get_relevant_passages(query)\n",
    "        relevant_passages_count = repository.count_relevant_documents(\n",
    "            passage_id, dataset_key\n",
    "        )\n",
    "\n",
    "        if relevant_passages_count == 0:\n",
    "            print(f\"ERROR NO RELEVANT PASSAGES - passage id {passage_id}\")\n",
    "            break\n",
    "\n",
    "        ndcg = retriever_evaluator.calculate_ndcg(result, passage_id)\n",
    "        mrr = retriever_evaluator.calculate_mrr(result, passage_id)\n",
    "        recall = retriever_evaluator.calculate_recall(\n",
    "            result, passage_id, relevant_passages_count\n",
    "        )\n",
    "        accuracy = retriever_evaluator.calculate_accuracy(result, passage_id)\n",
    "\n",
    "        ndcgs.append(ndcg)\n",
    "        mrrs.append(mrr)\n",
    "        recalls.append(recall)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    scores[\"ndcg\"] = sum(ndcgs) / len(ndcgs)\n",
    "    scores[\"mrr\"] = sum(mrrs) / len(mrrs)\n",
    "    scores[\"recall\"] = sum(recalls) / len(recalls)\n",
    "    scores[\"accuracy\"] = sum(accuracies) / len(accuracies)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def run_polqa_evaluation(dataset: list[DatasetEntry], repository: Repository, retriever: Retriever, dataset_key: str):\n",
    "    scores: Dict[str, float] = {}\n",
    "\n",
    "    ndcgs = []\n",
    "    mrrs = []\n",
    "    recalls = []\n",
    "    accuracies = []\n",
    "\n",
    "    for entry in dataset:\n",
    "        passage_id = entry.passage_id\n",
    "        query = entry.question\n",
    "\n",
    "        result = retriever.get_relevant_passages(query)\n",
    "        relevant_passages_count = repository.count_relevant_documents(\n",
    "            passage_id, dataset_key\n",
    "        )\n",
    "\n",
    "        if relevant_passages_count == 0:\n",
    "            print(f\"ERROR NO RELEVANT PASSAGES - passage id {passage_id}\")\n",
    "            break\n",
    "\n",
    "        ndcg = retriever_evaluator.calculate_ndcg(result, passage_id)\n",
    "        mrr = retriever_evaluator.calculate_mrr(result, passage_id)\n",
    "        recall = retriever_evaluator.calculate_recall(\n",
    "            result, passage_id, relevant_passages_count\n",
    "        )\n",
    "        accuracy = retriever_evaluator.calculate_accuracy(result, passage_id)\n",
    "        if recall < 0:\n",
    "            break\n",
    "        if recall > 1:\n",
    "            print(dataset_key, passage_id, recall, relevant_passages_count)\n",
    "            for passage, _ in result.passages:\n",
    "                print(passage)\n",
    "            break\n",
    "\n",
    "        ndcgs.append(ndcg)\n",
    "        mrrs.append(mrr)\n",
    "        recalls.append(recall)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    scores[\"ndcg\"] = sum(ndcgs) / len(ndcgs)\n",
    "    scores[\"mrr\"] = sum(mrrs) / len(mrrs)\n",
    "    scores[\"recall\"] = sum(recalls) / len(recalls)\n",
    "    scores[\"accuracy\"] = sum(accuracies) / len(accuracies)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repository.es_repository import ESRepository\n",
    "from repository.qdrant_repository import QdrantRepository\n",
    "from rerankers.hf_reranker import HFReranker\n",
    "from retrievers.es_retriever import ESRetriever\n",
    "\n",
    "def run_es_evaluations(combinations, datasets, reranker: HFReranker = None):\n",
    "    es_scores: Dict[str, float] = {}\n",
    "    poquad_dataset, polqa_dataset  = datasets\n",
    "\n",
    "    for index, dataset_key in combinations:\n",
    "        repository = ESRepository(es_client, index, cache)\n",
    "        retriever = ESRetriever(repository, dataset_key, reranker)\n",
    "\n",
    "        selected_dataset = poquad_dataset if \"poquad\" in dataset_key else polqa_dataset\n",
    "        evaluator_func = run_poquad_evaluation if \"poquad\" in dataset_key else run_polqa_evaluation\n",
    "\n",
    "        scores = evaluator_func(selected_dataset, repository, retriever, dataset_key)\n",
    "        es_scores[f\"{index}-{dataset_key}\"] = scores\n",
    "\n",
    "        print(f\"{index}-{dataset_key}\")\n",
    "        print(scores)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    return es_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from common.names import PASSAGE_PREFIX_MAP, QUERY_PREFIX_MAP\n",
    "from repository.qdrant_repository import QdrantRepository\n",
    "from retrievers.qdrant_retriever import QdrantRetriever\n",
    "\n",
    "\n",
    "def run_qdrant_evaluations(combinations, datasets, reranker: HFReranker = None):\n",
    "    qdrant_scores: Dict[str, float] = {}\n",
    "\n",
    "    poquad_dataset, polqa_dataset  = datasets\n",
    "\n",
    "    for model, distance, dataset_key in combinations:\n",
    "        passage_prefix = PASSAGE_PREFIX_MAP[model]\n",
    "        query_prefix = QUERY_PREFIX_MAP[model]\n",
    "\n",
    "        repository = QdrantRepository.get_repository(\n",
    "            qdrant_client, model, distance, cache, passage_prefix, query_prefix\n",
    "        )\n",
    "        retriever = QdrantRetriever(repository, dataset_key, reranker)\n",
    "\n",
    "        selected_dataset = poquad_dataset if \"poquad\" in dataset_key else polqa_dataset\n",
    "        evaluator_func = (\n",
    "            run_poquad_evaluation if \"poquad\" in dataset_key else run_polqa_evaluation\n",
    "        )\n",
    "\n",
    "        scores = evaluator_func(selected_dataset, repository, retriever, dataset_key)\n",
    "        qdrant_scores[f\"{model}-{distance}-{dataset_key}\"] = scores\n",
    "\n",
    "        cache.set(f\"qdrant_scores:{model}-{distance}-{dataset_key}\", json.dumps(scores))\n",
    "\n",
    "        print(f\"{model}-{distance}-{dataset_key}\")\n",
    "        print(scores)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    return qdrant_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "es_scores = None\n",
    "cached_es_scores = cache.get(\"score:es\")\n",
    "\n",
    "if (cached_es_scores is not None):\n",
    "    es_scores = json.loads(cached_es_scores)\n",
    "else:\n",
    "    es_scores = run_es_evaluations(es_index_combinations, (poquad_dataset, polqa_dataset))\n",
    "    cache.set(\"score:es\", json.dumps(es_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_scores = None\n",
    "cached_qdrant_scores = cache.get(\"score:qdrant\")\n",
    "\n",
    "if cached_qdrant_scores is not None:\n",
    "    qdrant_scores = json.loads(cached_qdrant_scores)\n",
    "else:\n",
    "    qdrant_scores = run_qdrant_evaluations(qdrant_model_combinations, (poquad_dataset, polqa_dataset))\n",
    "    cache.set(\"score:qdrant\", json.dumps(qdrant_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Distance\n",
    "\n",
    "hybrid_combinations = [\n",
    "    (\n",
    "        \"clarin-pl-poquad-100000\",\n",
    "        \"morfologik_index\",\n",
    "        \"intfloat/multilingual-e5-large\",\n",
    "        Distance.COSINE,\n",
    "    ),\n",
    "    (\n",
    "        \"ipipan-polqa-1000\",\n",
    "        \"morfologik_index\",\n",
    "        \"sdadas/mmlw-retrieval-roberta-large\",\n",
    "        Distance.COSINE,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repository import qdrant_repository\n",
    "from repository.es_repository import ESRepository\n",
    "from retrievers.es_retriever import ESRetriever\n",
    "from retrievers.hybrid_retriever import HybridRetriever\n",
    "\n",
    "\n",
    "def run_hybrid_evaluations(combinations, datasets, alphas: list[int], reranker: HFReranker = None):\n",
    "    hybrid_scores: Dict[str, float] = {}\n",
    "\n",
    "    poquad_dataset, polqa_dataset  = datasets\n",
    "\n",
    "    for dataset_key, es_index, qdrant_model, qdrant_distance in combinations:\n",
    "        for alpha in alphas:\n",
    "            es_repository = ESRepository(es_client, es_index, cache)\n",
    "\n",
    "            passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "            query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "            qdrant_repository = QdrantRepository.get_repository(\n",
    "                qdrant_client,\n",
    "                qdrant_model,\n",
    "                qdrant_distance,\n",
    "                cache,\n",
    "                passage_prefix,\n",
    "                query_prefix,\n",
    "            )\n",
    "\n",
    "            retriever = HybridRetriever(\n",
    "                es_repository, qdrant_repository, dataset_key, alpha, reranker\n",
    "            )\n",
    "\n",
    "            selected_dataset = poquad_dataset if \"poquad\" in dataset_key else polqa_dataset\n",
    "            evaluator_func = (\n",
    "                run_poquad_evaluation if \"poquad\" in dataset_key else run_polqa_evaluation\n",
    "            )\n",
    "\n",
    "            scores = evaluator_func(\n",
    "                selected_dataset, es_repository, retriever, dataset_key\n",
    "            )\n",
    "            hybrid_scores[\n",
    "                f\"{es_index}-{qdrant_model}-{qdrant_distance}-{dataset_key}-{alpha}\"\n",
    "            ] = scores\n",
    "\n",
    "            print(f\"{es_index}-{qdrant_model}-{qdrant_distance}-{dataset_key}-{alpha}\")\n",
    "            print(scores)\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "    return hybrid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_scores = None\n",
    "cached_hybrid_scores = cache.get(\"score:hybrid\")\n",
    "\n",
    "if cached_hybrid_scores is not None:\n",
    "    hybrid_scores = json.loads(cached_hybrid_scores)\n",
    "else:\n",
    "    hybrid_scores = run_hybrid_evaluations(hybrid_combinations, (poquad_dataset, polqa_dataset), [0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75])\n",
    "    cache.set(\"score:hybrid\", json.dumps(hybrid_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_reranker_combinations = [('morfologik_index', 'clarin-pl-poquad-100000'), ('basic_index', 'clarin-pl-poquad-500'), ('morfologik_index', 'ipipan-polqa-1000'), ('basic_index', 'ipipan-polqa-500')]\n",
    "qdrant_reranker_combinations = [(\"intfloat/multilingual-e5-large\", Distance.COSINE, \"clarin-pl-poquad-100000\"), (\"sdadas/mmlw-roberta-large\", Distance.EUCLID, \"clarin-pl-poquad-500\"), (\"sdadas/mmlw-retrieval-roberta-large\", Distance.COSINE, \"ipipan-polqa-1000\"), (\"sdadas/mmlw-roberta-large\", Distance.EUCLID, \"ipipan-polqa-500\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.names import RERANKER_MODEL_NAMES\n",
    "\n",
    "\n",
    "def run_reranker_evaluations():\n",
    "    reranker_scores: Dict[str, float] = {}\n",
    "\n",
    "    for reranker_model in RERANKER_MODEL_NAMES:\n",
    "        reranker = HFReranker(reranker_model, cache)\n",
    "\n",
    "        es_reranker_scores = run_es_evaluations(\n",
    "            es_reranker_combinations, (poquad_dataset, polqa_dataset), reranker\n",
    "        )\n",
    "        for key, value in es_reranker_scores.items():\n",
    "            reranker_scores[f\"{key}-{reranker_model}\"] = value\n",
    "\n",
    "        qdrant_reranker_scores = run_qdrant_evaluations(\n",
    "            qdrant_reranker_combinations,\n",
    "            (poquad_dataset, polqa_dataset),\n",
    "            reranker,\n",
    "        )\n",
    "        for key, value in qdrant_reranker_scores.items():\n",
    "            reranker_scores[f\"{key}-{reranker_model}\"] = value\n",
    "\n",
    "        hybrid_reranker_scores = run_hybrid_evaluations(\n",
    "            hybrid_combinations,\n",
    "            (poquad_dataset, polqa_dataset),\n",
    "            [0.25, 0.5, 0.75],\n",
    "            reranker,\n",
    "        )\n",
    "        for key, value in hybrid_reranker_scores.items():\n",
    "            reranker_scores[f\"{key}-{reranker_model}\"] = value\n",
    "\n",
    "    return reranker_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_scores = None\n",
    "cached_reranker_scores = cache.get(\"score:reranker\")\n",
    "\n",
    "if cached_reranker_scores is not None:\n",
    "    reranker_scores = json.loads(cached_reranker_scores)\n",
    "else:\n",
    "    reranker_scores = run_reranker_evaluations()\n",
    "    cache.set(\"score:reranker\", json.dumps(reranker_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from repository.qdrant_openai_repository import QdrantOpenAIRepository\n",
    "from retrievers.qdrant_retriever import QdrantRetriever\n",
    "\n",
    "\n",
    "def run_openai_evaluations(combinations, datasets):\n",
    "    openai_scores: Dict[str, float] = {}\n",
    "\n",
    "    poquad_dataset, polqa_dataset = datasets\n",
    "\n",
    "    for model, distance, dataset_key in combinations:\n",
    "        repository = QdrantOpenAIRepository.get_repository(\n",
    "            qdrant_client, model, distance, cache\n",
    "        )\n",
    "        retriever = QdrantRetriever(repository, dataset_key)\n",
    "\n",
    "        selected_dataset = poquad_dataset if \"poquad\" in dataset_key else polqa_dataset\n",
    "        evaluator_func = (\n",
    "            run_poquad_evaluation if \"poquad\" in dataset_key else run_polqa_evaluation\n",
    "        )\n",
    "\n",
    "        scores = evaluator_func(selected_dataset, repository, retriever, dataset_key)\n",
    "        openai_scores[f\"{model}-{distance}-{dataset_key}\"] = scores\n",
    "\n",
    "        cache.set(\n",
    "            f\"openai_scores:{model}-{distance}-{dataset_key}\", json.dumps(scores)\n",
    "        )\n",
    "\n",
    "        print(f\"{model}-{distance}-{dataset_key}\")\n",
    "        print(scores)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    return openai_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "text-embedding-3-large-Cosine-ipipan-polqa-500\n",
      "{'ndcg': 0.9267680443358624, 'mrr': 0.9123190476190476, 'recall': 0.973, 'accuracy': 0.872}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "text-embedding-3-large-Euclid-ipipan-polqa-500\n",
      "{'ndcg': 0.9307503429221868, 'mrr': 0.9163190476190476, 'recall': 0.977, 'accuracy': 0.876}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "text-embedding-3-large-Cosine-ipipan-polqa-1000\n",
      "{'ndcg': 0.9346719575285637, 'mrr': 0.9195301587301588, 'recall': 0.98, 'accuracy': 0.88}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "text-embedding-3-large-Euclid-ipipan-polqa-1000\n",
      "{'ndcg': 0.9326480271898715, 'mrr': 0.9175079365079365, 'recall': 0.978, 'accuracy': 0.878}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "text-embedding-3-large-Cosine-ipipan-polqa-2000\n",
      "{'ndcg': 0.9346480271898715, 'mrr': 0.9195079365079365, 'recall': 0.98, 'accuracy': 0.88}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "text-embedding-3-large-Euclid-ipipan-polqa-2000\n",
      "{'ndcg': 0.9349098866970144, 'mrr': 0.9198412698412699, 'recall': 0.98, 'accuracy': 0.88}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "text-embedding-3-large-Cosine-ipipan-polqa-100000\n",
      "{'ndcg': 0.9346480271898715, 'mrr': 0.9195079365079365, 'recall': 0.98, 'accuracy': 0.88}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "text-embedding-3-large-Euclid-ipipan-polqa-100000\n",
      "{'ndcg': 0.9346480271898715, 'mrr': 0.9195079365079365, 'recall': 0.98, 'accuracy': 0.88}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "text-embedding-3-large-Cosine-clarin-pl-poquad-500\n",
      "{'ndcg': 0.7867749644892975, 'mrr': 0.7795269841269841, 'recall': 0.7641214285714286, 'accuracy': 0.722}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "text-embedding-3-large-Euclid-clarin-pl-poquad-500\n",
      "{'ndcg': 0.7884040668888682, 'mrr': 0.781365873015873, 'recall': 0.7702214285714285, 'accuracy': 0.724}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "text-embedding-3-large-Cosine-clarin-pl-poquad-1000\n",
      "{'ndcg': 0.8035577617299292, 'mrr': 0.7777428571428572, 'recall': 0.8665, 'accuracy': 0.716}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "text-embedding-3-large-Euclid-clarin-pl-poquad-1000\n",
      "{'ndcg': 0.7966292427463112, 'mrr': 0.7711095238095238, 'recall': 0.8611666666666666, 'accuracy': 0.71}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "text-embedding-3-large-Cosine-clarin-pl-poquad-2000\n",
      "{'ndcg': 0.8143334846169674, 'mrr': 0.7843134920634921, 'recall': 0.906, 'accuracy': 0.714}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "text-embedding-3-large-Euclid-clarin-pl-poquad-2000\n",
      "{'ndcg': 0.8106236366250755, 'mrr': 0.780702380952381, 'recall': 0.901, 'accuracy': 0.71}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "text-embedding-3-large-Cosine-clarin-pl-poquad-100000\n",
      "{'ndcg': 0.8121531161405217, 'mrr': 0.7814547619047619, 'recall': 0.906, 'accuracy': 0.71}\n",
      "\n",
      "\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "text-embedding-3-large-Euclid-clarin-pl-poquad-100000\n",
      "{'ndcg': 0.8062628932621313, 'mrr': 0.7762603174603174, 'recall': 0.898, 'accuracy': 0.706}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "openai_scores = None\n",
    "cached_openai_scores = cache.get(\"score:openai\")\n",
    "\n",
    "if cached_openai_scores is not None:\n",
    "    openai_scores = json.loads(cached_openai_scores)\n",
    "else:\n",
    "    openai_scores = run_openai_evaluations(\n",
    "        openai_model_combinations, (poquad_dataset, polqa_dataset)\n",
    "    )\n",
    "    cache.set(\"score:openai\", json.dumps(openai_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results as csv\n",
    "import csv\n",
    "\n",
    "def save_scores_to_csv(scores, filename):\n",
    "    with open(filename, mode='w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['model', 'ndcg', 'mrr', 'recall', 'accuracy', 'sum', 'avg'])\n",
    "        for key, value in scores.items():\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    key,\n",
    "                    str(value[\"ndcg\"]).replace(\".\", \",\"),\n",
    "                    str(value[\"mrr\"]).replace(\".\", \",\"),\n",
    "                    str(value[\"recall\"]).replace(\".\", \",\"),\n",
    "                    str(value[\"accuracy\"]).replace(\".\", \",\"),\n",
    "                    str(value[\"ndcg\"] + value[\"mrr\"] + value[\"recall\"] + value[\"accuracy\"]).replace(\".\", \",\"),\n",
    "                    str((value[\"ndcg\"] + value[\"mrr\"] + value[\"recall\"] + value[\"accuracy\"]) / 4).replace(\".\", \",\"),\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_poquad_scores = {key: value for key, value in es_scores.items() if \"poquad\" in key}\n",
    "es_polqa_scores = {key: value for key, value in es_scores.items() if \"polqa\" in key}\n",
    "\n",
    "qdrant_poquad_scores = {key: value for key, value in qdrant_scores.items() if \"poquad\" in key}\n",
    "qdrant_polqa_scores = {key: value for key, value in qdrant_scores.items() if \"polqa\" in key}\n",
    "\n",
    "hybrid_poquad_scores = {key: value for key, value in hybrid_scores.items() if \"poquad\" in key}\n",
    "hybrid_polqa_scores = {key: value for key, value in hybrid_scores.items() if \"polqa\" in key}\n",
    "\n",
    "reranker_poquad_scores = {key: value for key, value in reranker_scores.items() if \"poquad\" in key}\n",
    "reranker_polqa_scores = {key: value for key, value in reranker_scores.items() if \"polqa\" in key}\n",
    "\n",
    "openai_poquad_scores = {key: value for key, value in openai_scores.items() if \"poquad\" in key}\n",
    "openai_polqa_scores = {key: value for key, value in openai_scores.items() if \"polqa\" in key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_scores_to_csv(es_poquad_scores, \"../../output/es_scores_poquad.csv\")\n",
    "save_scores_to_csv(es_polqa_scores, \"../../output/es_scores_polqa.csv\")\n",
    "\n",
    "save_scores_to_csv(qdrant_poquad_scores, \"../../output/qdrant_scores_poquad.csv\")\n",
    "save_scores_to_csv(qdrant_polqa_scores, \"../../output/qdrant_scores_polqa.csv\")\n",
    "\n",
    "save_scores_to_csv(hybrid_poquad_scores, \"../../output/hybrid_scores_poquad.csv\")\n",
    "save_scores_to_csv(hybrid_polqa_scores, \"../../output/hybrid_scores_polqa.csv\")\n",
    "\n",
    "save_scores_to_csv(reranker_poquad_scores, \"../../output/reranker_scores_poquad.csv\")\n",
    "save_scores_to_csv(reranker_polqa_scores, \"../../output/reranker_scores_polqa.csv\")\n",
    "\n",
    "save_scores_to_csv(openai_poquad_scores, \"../../output/openai_scores_poquad.csv\")\n",
    "save_scores_to_csv(openai_polqa_scores, \"../../output/openai_scores_polqa.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
