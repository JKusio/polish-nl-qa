{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94fbd43",
   "metadata": {},
   "source": [
    "# RAGAS V2 Re-evaluation Notebook\n",
    "\n",
    "This notebook regenerates answers for instruction models with improved prompts and evaluates them using RAGAS V2.\n",
    "\n",
    "Goals:\n",
    "1. Regenerate answers for all instruction models (using cached retrievals)\n",
    "2. Use improved prompt to get direct answers without reasoning\n",
    "3. Evaluate with RAGAS V2 for better scoring\n",
    "4. Compare results with original RAGAS scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1629824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc6ac960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v7/y_kh76zd2s55cwm9bpl4cg500000gn/T/ipykernel_28191/2018490365.py:5: UserWarning: Qdrant client version 1.14.3 is incompatible with server version 1.3.1. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from qdrant_client import QdrantClient\n",
    "from cache.cache import Cache\n",
    "\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "es_client = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"],\n",
    ")\n",
    "cache = Cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eab61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.names import (\n",
    "    OPENAI_EMBEDDING_MODEL_NAMES,\n",
    "    PASSAGE_PREFIX_MAP,\n",
    "    QUERY_PREFIX_MAP,\n",
    "    INST_MODEL_PATHS,\n",
    "    DATASET_SEED,\n",
    ")\n",
    "from repository.es_repository import ESRepository\n",
    "from repository.qdrant_openai_repository import QdrantOpenAIRepository\n",
    "from repository.qdrant_repository import QdrantRepository\n",
    "from qdrant_client.models import Distance\n",
    "\n",
    "from rerankers.hf_reranker import HFReranker\n",
    "from retrievers.es_retriever import ESRetriever\n",
    "from retrievers.hybrid_retriever import HybridRetriever\n",
    "from retrievers.qdrant_retriever import QdrantRetriever\n",
    "from retrievers.retriever import Retriever\n",
    "from generators.instruction_generator import InstructionGenerator\n",
    "from generators.openai_generator import OpenAIGenerator\n",
    "\n",
    "from dataset.polqa_dataset_getter import PolqaDatasetGetter\n",
    "from dataset.poquad_dataset_getter import PoquadDatasetGetter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbcbe77",
   "metadata": {},
   "source": [
    "## Define Retriever Functions\n",
    "\n",
    "Using the same retrievers as in the manual evaluation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fca1681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_poquad_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"clarin-pl-poquad-100000\"\n",
    "    es_index = \"morfologik_index\"\n",
    "    qdrant_model = \"intfloat/multilingual-e5-large\"\n",
    "    reranker_model = \"sdadas/polish-reranker-large-ranknet\"\n",
    "    alpha = 0.5\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "    passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "    query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "    qdrant_repository = QdrantRepository.get_repository(\n",
    "        qdrant_client,\n",
    "        qdrant_model,\n",
    "        Distance.COSINE,\n",
    "        cache,\n",
    "        passage_prefix,\n",
    "        query_prefix,\n",
    "    )\n",
    "    reranker = HFReranker(reranker_model, cache)\n",
    "\n",
    "    retriever = HybridRetriever(\n",
    "        es_repository, qdrant_repository, dataset_key, alpha, reranker\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"morfologik_index-intfloat/multilingual-e5-large-Cosine-clarin-pl-poquad-100000-0.5-sdadas/polish-reranker-large-ranknet\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_50p_poquad_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"clarin-pl-poquad-1000\"\n",
    "    qdrant_model = \"sdadas/mmlw-retrieval-roberta-large\"\n",
    "\n",
    "    passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "    query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "    qdrant_repository = QdrantRepository.get_repository(\n",
    "        qdrant_client,\n",
    "        qdrant_model,\n",
    "        Distance.EUCLID,\n",
    "        cache,\n",
    "        passage_prefix,\n",
    "        query_prefix,\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(qdrant_repository, dataset_key)\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"sdadas/mmlw-retrieval-roberta-large-Euclid-clarin-pl-poquad-1000\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_worst_poquad_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"clarin-pl-poquad-500\"\n",
    "    es_index = \"basic_index\"\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "\n",
    "    retriever = ESRetriever(es_repository, dataset_key)\n",
    "\n",
    "    return (retriever, \"basic_index-clarin-pl-poquad-500\")\n",
    "\n",
    "\n",
    "def get_best_poquad_openai_retriever() -> tuple[Retriever, str]:\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.COSINE, cache\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(repository, \"clarin-pl-poquad-2000\")\n",
    "\n",
    "    return (retriever, \"text-embedding-3-large-Cosine-clarin-pl-poquad-2000\")\n",
    "\n",
    "\n",
    "def get_worst_poquad_openai_retriever() -> tuple[Retriever, str]:\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.COSINE, cache\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(repository, \"clarin-pl-poquad-500\")\n",
    "\n",
    "    return (retriever, \"text-embedding-3-large-Cosine-clarin-pl-poquad-500\")\n",
    "\n",
    "\n",
    "def get_best_polqa_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"ipipan-polqa-1000\"\n",
    "    es_index = \"morfologik_index\"\n",
    "    qdrant_model = \"sdadas/mmlw-retrieval-roberta-large\"\n",
    "    reranker_model = \"sdadas/polish-reranker-large-ranknet\"\n",
    "    alpha = 0.75\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "    passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "    query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "    qdrant_repository = QdrantRepository.get_repository(\n",
    "        qdrant_client,\n",
    "        qdrant_model,\n",
    "        Distance.COSINE,\n",
    "        cache,\n",
    "        passage_prefix,\n",
    "        query_prefix,\n",
    "    )\n",
    "    reranker = HFReranker(reranker_model, cache)\n",
    "\n",
    "    retriever = HybridRetriever(\n",
    "        es_repository, qdrant_repository, dataset_key, alpha, reranker\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"morfologik_index-sdadas/mmlw-retrieval-roberta-large-Cosine-ipipan-polqa-1000-0.75-sdadas/polish-reranker-large-ranknet\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_50p_polqa_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"ipipan-polqa-1000\"\n",
    "    es_index = \"morfologik_index\"\n",
    "    qdrant_model = \"sdadas/mmlw-retrieval-roberta-large\"\n",
    "    alpha = 0.75\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "    passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "    query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "    qdrant_repository = QdrantRepository.get_repository(\n",
    "        qdrant_client,\n",
    "        qdrant_model,\n",
    "        Distance.COSINE,\n",
    "        cache,\n",
    "        passage_prefix,\n",
    "        query_prefix,\n",
    "    )\n",
    "\n",
    "    retriever = HybridRetriever(es_repository, qdrant_repository, dataset_key, alpha)\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"morfologik_index-sdadas/mmlw-retrieval-roberta-large-Cosine-ipipan-polqa-1000-0.75\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_worst_polqa_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"ipipan-polqa-500\"\n",
    "    es_index = \"basic_index\"\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "\n",
    "    retriever = ESRetriever(es_repository, dataset_key)\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"basic_index-ipipan-polqa-500\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_best_polqa_openai_retriever() -> tuple[Retriever, str]:\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.EUCLID, cache\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(repository, \"ipipan-polqa-2000\")\n",
    "\n",
    "    return (retriever, \"text-embedding-3-large-Euclid-ipipan-polqa-2000\")\n",
    "\n",
    "\n",
    "def get_worst_polqa_openai_retriever() -> tuple[Retriever, str]:\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.COSINE, cache\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(repository, \"ipipan-polqa-500\")\n",
    "\n",
    "    return (retriever, \"text-embedding-3-large-Cosine-ipipan-polqa-500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07498b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "poquad_retriever_functions = [\n",
    "    get_best_poquad_retriever,\n",
    "    get_50p_poquad_retriever,\n",
    "    get_worst_poquad_retriever,\n",
    "]\n",
    "\n",
    "poquad_openai_retriever_functions = [\n",
    "    get_best_poquad_openai_retriever,\n",
    "    get_worst_poquad_openai_retriever,\n",
    "]\n",
    "\n",
    "polqa_retriever_functions = [\n",
    "    get_best_polqa_retriever,\n",
    "    get_50p_polqa_retriever,\n",
    "    get_worst_polqa_retriever,\n",
    "]\n",
    "\n",
    "polqa_openai_retriever_functions = [\n",
    "    get_best_polqa_openai_retriever,\n",
    "    get_worst_polqa_openai_retriever,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62ff58",
   "metadata": {},
   "source": [
    "## Load Test Datasets\n",
    "\n",
    "Using the same 100-question subset as in manual evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dc12857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoQuAD dataset: 100 questions\n",
      "PolQA dataset: 100 questions\n"
     ]
    }
   ],
   "source": [
    "poquad_dataset_getter = PoquadDatasetGetter()\n",
    "polqa_dataset_getter = PolqaDatasetGetter()\n",
    "\n",
    "poquad_dataset = poquad_dataset_getter.get_random_n_test(500, DATASET_SEED)[:100]\n",
    "polqa_dataset = polqa_dataset_getter.get_random_n_test(500, DATASET_SEED)[:100]\n",
    "\n",
    "print(f\"PoQuAD dataset: {len(poquad_dataset)} questions\")\n",
    "print(f\"PolQA dataset: {len(polqa_dataset)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36cc529",
   "metadata": {},
   "source": [
    "## Initialize RAGAS V2 Evaluator\n",
    "\n",
    "We'll use the improved RAGAS V2 evaluator with better scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "399c8be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "RAGAS V2 evaluator initialized with PLLuM-12B for question generation!\n",
      "RAGAS V2 evaluator initialized with PLLuM-12B for question generation!\n"
     ]
    }
   ],
   "source": [
    "from evaluation.ragas_evaulator_v2 import RAGASEvaluatorV2\n",
    "from vectorizer.hf_vectorizer import HFVectorizer\n",
    "\n",
    "# Initialize vectorizer for RAGAS\n",
    "vectorizer = HFVectorizer(\"intfloat/multilingual-e5-large\", cache)\n",
    "\n",
    "# Initialize RAGAS V2\n",
    "ragas_evaluator = RAGASEvaluatorV2(\n",
    "    reranker_model_name=\"sdadas/polish-reranker-large-ranknet\",\n",
    "    cache=cache,\n",
    "    generator_model_name=INST_MODEL_PATHS[2],  # PLLuM-12B - best for Polish question generation\n",
    "    vectorizer=vectorizer,\n",
    ")\n",
    "\n",
    "print(\"RAGAS V2 evaluator initialized with PLLuM-12B for question generation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68332a8",
   "metadata": {},
   "source": [
    "## Define Evaluation Function\n",
    "\n",
    "This function will:\n",
    "1. Use cached retrieval results\n",
    "2. Regenerate answers with instruction_v2 hash (new prompt)\n",
    "3. Evaluate with RAGAS V2 including answer correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01bacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "\n",
    "def clean_text_for_csv(text):\n",
    "    \"\"\"Clean text to be CSV-safe by removing newlines and extra whitespace\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # Convert to string and replace newlines with spaces\n",
    "    cleaned = str(text).replace('\\n', ' ').replace('\\r', ' ')\n",
    "    # Replace multiple spaces with single space\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    return cleaned\n",
    "\n",
    "def create_safe_filename_v2(retriever_name, generator_name, generator_type, dataset_name, n):\n",
    "    \"\"\"Create a safe filename from the configuration using hash to avoid collisions\"\"\"\n",
    "    # Clean names for filename\n",
    "    clean_retriever = clean_text_for_csv(retriever_name).replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "    clean_generator = clean_text_for_csv(generator_name).replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "    \n",
    "    # If retriever name is too long, use first 40 chars + hash of full name\n",
    "    if len(clean_retriever) > 50:\n",
    "        # Create a short hash of the full retriever name for uniqueness\n",
    "        retriever_hash = hashlib.md5(clean_retriever.encode()).hexdigest()[:8]\n",
    "        safe_retriever = f\"{clean_retriever[:40]}_{retriever_hash}\"\n",
    "    else:\n",
    "        safe_retriever = clean_retriever\n",
    "    \n",
    "    # Truncate generator name if needed\n",
    "    safe_generator = clean_generator[:30]\n",
    "    \n",
    "    return f\"ragas_v2_{dataset_name}_{safe_retriever}_{safe_generator}_{generator_type}_n{n}.csv\"\n",
    "\n",
    "def evaluate_generator_ragas_v2(\n",
    "    retriever_name: str,\n",
    "    retriever,\n",
    "    generator_name: str,\n",
    "    generator,\n",
    "    dataset,\n",
    "    dataset_name: str,\n",
    "    n: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate generator using RAGAS V2 with answer correctness\n",
    "    Also returns data for manual evaluation files\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    manual_eval_rows = []\n",
    "    \n",
    "    for i, entry in enumerate(tqdm(dataset, desc=f\"{dataset_name} - {generator_name[:30]}\")):\n",
    "        question = entry.question\n",
    "        correct_passage_id = entry.passage_id\n",
    "        correct_answers = entry.answers\n",
    "        \n",
    "        # Get retrieval results (cached)\n",
    "        retriever_result = retriever.get_relevant_passages(question)\n",
    "        passages = [passage for (passage, _) in retriever_result.passages]\n",
    "        top_n_passages = passages[:n]\n",
    "        \n",
    "        # Generate answer (will use instruction_v3 cache - NEW ANSWERS!)\n",
    "        answer = generator.generate_answer(question, top_n_passages)\n",
    "        \n",
    "        # Check if correct passage is in top n\n",
    "        retrieved_ids = [passage.id for passage in top_n_passages]\n",
    "        has_correct_passages = str(correct_passage_id in retrieved_ids).upper()\n",
    "        \n",
    "        # Evaluate with RAGAS V2 including correctness\n",
    "        ragas_score = ragas_evaluator.ragas(\n",
    "            retriever_result,\n",
    "            correct_passage_id,\n",
    "            answer,\n",
    "            correct_answers=correct_answers\n",
    "        )\n",
    "        \n",
    "        # Also get individual metrics for analysis\n",
    "        faithfulness = ragas_evaluator.faithfulness(retriever_result, answer)\n",
    "        answer_relevance = ragas_evaluator.answer_relevance(question, answer)\n",
    "        answer_correctness = ragas_evaluator.answer_correctness(answer, correct_answers)\n",
    "        context_recall = ragas_evaluator.context_recall(retriever_result, correct_passage_id)\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'correct_answers': ' | '.join(correct_answers) if isinstance(correct_answers, list) else correct_answers,\n",
    "            'ragas_v2': ragas_score,\n",
    "            'faithfulness': faithfulness,\n",
    "            'answer_relevance': answer_relevance,\n",
    "            'answer_correctness': answer_correctness,\n",
    "            'context_recall': context_recall,\n",
    "        })\n",
    "        \n",
    "        # Prepare row for manual evaluation file\n",
    "        question_id = f\"{dataset_name}_q{i+1}\"\n",
    "        question_text_clean = clean_text_for_csv(question)\n",
    "        answer_clean = clean_text_for_csv(answer)\n",
    "        \n",
    "        if isinstance(correct_answers, list):\n",
    "            correct_answer_text = \" | \".join([clean_text_for_csv(ans) for ans in correct_answers])\n",
    "        else:\n",
    "            correct_answer_text = clean_text_for_csv(str(correct_answers))\n",
    "        \n",
    "        manual_eval_rows.append([\n",
    "            question_text_clean,\n",
    "            question_id,\n",
    "            has_correct_passages,\n",
    "            f\"{ragas_score:.4f}\",  # RAGAS V2 overall score\n",
    "            f\"{faithfulness:.4f}\",  # Faithfulness score\n",
    "            f\"{answer_relevance:.4f}\",  # Answer relevance score\n",
    "            f\"{answer_correctness:.4f}\",  # Answer correctness score\n",
    "            f\"{context_recall:.4f}\",  # Context recall score\n",
    "            answer_clean,\n",
    "            correct_answer_text,\n",
    "            \"\"  # Empty result column for manual evaluation\n",
    "        ])\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_ragas = sum(r['ragas_v2'] for r in results) / len(results)\n",
    "    avg_faithfulness = sum(r['faithfulness'] for r in results) / len(results)\n",
    "    avg_relevance = sum(r['answer_relevance'] for r in results) / len(results)\n",
    "    avg_correctness = sum(r['answer_correctness'] for r in results) / len(results)\n",
    "    avg_recall = sum(r['context_recall'] for r in results) / len(results)\n",
    "    \n",
    "    summary = {\n",
    "        'retriever': retriever_name,\n",
    "        'generator': generator_name,\n",
    "        'dataset': dataset_name,\n",
    "        'n': n,\n",
    "        'ragas_v2': avg_ragas,\n",
    "        'faithfulness': avg_faithfulness,\n",
    "        'answer_relevance': avg_relevance,\n",
    "        'answer_correctness': avg_correctness,\n",
    "        'context_recall': avg_recall,\n",
    "        'num_questions': len(results)\n",
    "    }\n",
    "    \n",
    "    return summary, results, manual_eval_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560bc1d3",
   "metadata": {},
   "source": [
    "## Run Evaluations for All Combinations\n",
    "\n",
    "This will evaluate all instruction models with n=[1, 5] for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428496c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING RAGAS V2 EVALUATION WITH NEW ANSWERS\n",
      "Cache key: instruction_v3 (will regenerate all answers)\n",
      "================================================================================\n",
      "\n",
      "### POQUAD DATASET ###\n",
      "\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "\n",
      "Retriever: morfologik_index-intfloat/multilingual-e5-large-Cosine-clarin-pl-poquad-100000-0.5-sdadas/polish-reranker-large-ranknet\n",
      "  Generator: ../../models/Bielik-11B-v2.2-Instruct-q4\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "\n",
      "Retriever: morfologik_index-intfloat/multilingual-e5-large-Cosine-clarin-pl-poquad-100000-0.5-sdadas/polish-reranker-large-ranknet\n",
      "  Generator: ../../models/Bielik-11B-v2.2-Instruct-q4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x12180b140>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jakubkusiowski/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluating with n=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "poquad - ../../models/Bielik-11B-v2.2-I: 100%|██████████| 100/100 [00:02<00:00, 33.48it/s]\n",
      "poquad - ../../models/Bielik-11B-v2.2-I: 100%|██████████| 100/100 [00:02<00:00, 33.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      RAGAS V2: 0.8021 | Correctness: 0.6654\n",
      "      Saved: ragas_v2_poquad_morfologik_index_intfloat_multilingual_e_01c8b7a6_.._.._models_Bielik_11B_v2.2_I_INST_n1.csv\n",
      "    Evaluating with n=5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "poquad - ../../models/Bielik-11B-v2.2-I:  19%|█▉        | 19/100 [00:00<00:00, 89.69it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"../../output/ragas_v2\", exist_ok=True)\n",
    "os.makedirs(\"../../output/ragas_v2/manual_eval\", exist_ok=True)\n",
    "\n",
    "# Track all summaries\n",
    "all_summaries = []\n",
    "all_detailed_results = []\n",
    "\n",
    "ns = [1, 5]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STARTING RAGAS V2 EVALUATION WITH NEW ANSWERS\")\n",
    "print(\"Cache key: instruction_v3 (will regenerate all answers)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# PoQuAD dataset evaluations\n",
    "print(\"\\n### POQUAD DATASET ###\\n\")\n",
    "for get_retriever in poquad_retriever_functions:\n",
    "    retriever, retriever_name = get_retriever()\n",
    "    print(f\"\\nRetriever: {retriever_name}\")\n",
    "    \n",
    "    for inst_model_path in INST_MODEL_PATHS:\n",
    "        print(f\"  Generator: {inst_model_path}\")\n",
    "        generator = InstructionGenerator(inst_model_path, cache)\n",
    "        \n",
    "        for n in ns:\n",
    "            print(f\"    Evaluating with n={n}...\")\n",
    "            summary, detailed, manual_rows = evaluate_generator_ragas_v2(\n",
    "                retriever_name,\n",
    "                retriever,\n",
    "                inst_model_path,\n",
    "                generator,\n",
    "                poquad_dataset,\n",
    "                \"poquad\",\n",
    "                n\n",
    "            )\n",
    "            \n",
    "            all_summaries.append(summary)\n",
    "            \n",
    "            # Add metadata to detailed results\n",
    "            for detail in detailed:\n",
    "                detail['retriever'] = retriever_name\n",
    "                detail['generator'] = inst_model_path\n",
    "                detail['dataset'] = 'poquad'\n",
    "                detail['n'] = n\n",
    "            all_detailed_results.extend(detailed)\n",
    "            \n",
    "            # Save manual evaluation file\n",
    "            filename = create_safe_filename_v2(retriever_name, inst_model_path, \"INST\", \"poquad\", n)\n",
    "            filepath = f\"../../output/ragas_v2/manual_eval/{filename}\"\n",
    "            \n",
    "            with open(filepath, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "                # Write metadata\n",
    "                file.write(f\"# RETRIEVER: {clean_text_for_csv(retriever_name)}\\n\")\n",
    "                file.write(f\"# GENERATOR: {clean_text_for_csv(inst_model_path)}\\n\")\n",
    "                file.write(f\"# TYPE: INST\\n\")\n",
    "                file.write(f\"# DATASET: poquad\\n\")\n",
    "                file.write(f\"# TOP_N: {n}\\n\")\n",
    "                file.write(f\"# CACHE_VERSION: instruction_v3\\n\")\n",
    "                file.write(f\"# RAGAS_VERSION: v2\\n\")\n",
    "                file.write(\"\\n\")\n",
    "                \n",
    "                # Write CSV data\n",
    "                writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "                writer.writerow([\"question\", \"question_id\", \"hasCorrectPassages\", \"ragas_v2_score\", \"faithfulness\", \"answer_relevance\", \"answer_correctness\", \"context_recall\", \"answer\", \"correct_answer\", \"manual_result\"])\n",
    "                writer.writerows(manual_rows)\n",
    "            \n",
    "            print(f\"      RAGAS V2: {summary['ragas_v2']:.4f} | Correctness: {summary['answer_correctness']:.4f}\")\n",
    "            print(f\"      Saved: {filename}\")\n",
    "\n",
    "# PoQuAD OpenAI evaluations\n",
    "print(\"\\n### POQUAD OPENAI DATASET ###\\n\")\n",
    "for get_retriever in poquad_openai_retriever_functions:\n",
    "    retriever, retriever_name = get_retriever()\n",
    "    print(f\"\\nRetriever: {retriever_name}\")\n",
    "    \n",
    "    print(f\"  Generator: gpt-4o-mini\")\n",
    "    generator = OpenAIGenerator(cache)\n",
    "    \n",
    "    for n in ns:\n",
    "        print(f\"    Evaluating with n={n}...\")\n",
    "        summary, detailed, manual_rows = evaluate_generator_ragas_v2(\n",
    "            retriever_name,\n",
    "            retriever,\n",
    "            \"gpt-4o-mini\",\n",
    "            generator,\n",
    "            poquad_dataset,\n",
    "            \"poquad_openai\",\n",
    "            n\n",
    "        )\n",
    "        \n",
    "        all_summaries.append(summary)\n",
    "        \n",
    "        for detail in detailed:\n",
    "            detail['retriever'] = retriever_name\n",
    "            detail['generator'] = 'gpt-4o-mini'\n",
    "            detail['dataset'] = 'poquad_openai'\n",
    "            detail['n'] = n\n",
    "        all_detailed_results.extend(detailed)\n",
    "        \n",
    "        # Save manual evaluation file\n",
    "        filename = create_safe_filename_v2(retriever_name, \"gpt-4o-mini\", \"INST\", \"poquad_openai\", n)\n",
    "        filepath = f\"../../output/ragas_v2/manual_eval/{filename}\"\n",
    "        \n",
    "        with open(filepath, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            file.write(f\"# RETRIEVER: {clean_text_for_csv(retriever_name)}\\n\")\n",
    "            file.write(f\"# GENERATOR: gpt-4o-mini\\n\")\n",
    "            file.write(f\"# TYPE: INST\\n\")\n",
    "            file.write(f\"# DATASET: poquad_openai\\n\")\n",
    "            file.write(f\"# TOP_N: {n}\\n\")\n",
    "            file.write(f\"# CACHE_VERSION: openai\\n\")\n",
    "            file.write(f\"# RAGAS_VERSION: v2\\n\")\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "            writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "            writer.writerow([\"question\", \"question_id\", \"hasCorrectPassages\", \"ragas_v2_score\", \"faithfulness\", \"answer_relevance\", \"answer_correctness\", \"context_recall\", \"answer\", \"correct_answer\", \"manual_result\"])\n",
    "            writer.writerows(manual_rows)\n",
    "        \n",
    "        print(f\"      RAGAS V2: {summary['ragas_v2']:.4f} | Correctness: {summary['answer_correctness']:.4f}\")\n",
    "        print(f\"      Saved: {filename}\")\n",
    "\n",
    "# PolQA dataset evaluations\n",
    "print(\"\\n### POLQA DATASET ###\\n\")\n",
    "for get_retriever in polqa_retriever_functions:\n",
    "    retriever, retriever_name = get_retriever()\n",
    "    print(f\"\\nRetriever: {retriever_name}\")\n",
    "    \n",
    "    for inst_model_path in INST_MODEL_PATHS:\n",
    "        print(f\"  Generator: {inst_model_path}\")\n",
    "        generator = InstructionGenerator(inst_model_path, cache)\n",
    "        \n",
    "        for n in ns:\n",
    "            print(f\"    Evaluating with n={n}...\")\n",
    "            summary, detailed, manual_rows = evaluate_generator_ragas_v2(\n",
    "                retriever_name,\n",
    "                retriever,\n",
    "                inst_model_path,\n",
    "                generator,\n",
    "                polqa_dataset,\n",
    "                \"polqa\",\n",
    "                n\n",
    "            )\n",
    "            \n",
    "            all_summaries.append(summary)\n",
    "            \n",
    "            for detail in detailed:\n",
    "                detail['retriever'] = retriever_name\n",
    "                detail['generator'] = inst_model_path\n",
    "                detail['dataset'] = 'polqa'\n",
    "                detail['n'] = n\n",
    "            all_detailed_results.extend(detailed)\n",
    "            \n",
    "            # Save manual evaluation file\n",
    "            filename = create_safe_filename_v2(retriever_name, inst_model_path, \"INST\", \"polqa\", n)\n",
    "            filepath = f\"../../output/ragas_v2/manual_eval/{filename}\"\n",
    "            \n",
    "            with open(filepath, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "                file.write(f\"# RETRIEVER: {clean_text_for_csv(retriever_name)}\\n\")\n",
    "                file.write(f\"# GENERATOR: {clean_text_for_csv(inst_model_path)}\\n\")\n",
    "                file.write(f\"# TYPE: INST\\n\")\n",
    "                file.write(f\"# DATASET: polqa\\n\")\n",
    "                file.write(f\"# TOP_N: {n}\\n\")\n",
    "                file.write(f\"# CACHE_VERSION: instruction_v3\\n\")\n",
    "                file.write(f\"# RAGAS_VERSION: v2\\n\")\n",
    "                file.write(\"\\n\")\n",
    "                \n",
    "                writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "                writer.writerow([\"question\", \"question_id\", \"hasCorrectPassages\", \"ragas_v2_score\", \"faithfulness\", \"answer_relevance\", \"answer_correctness\", \"context_recall\", \"answer\", \"correct_answer\", \"manual_result\"])\n",
    "                writer.writerows(manual_rows)\n",
    "            \n",
    "            print(f\"      RAGAS V2: {summary['ragas_v2']:.4f} | Correctness: {summary['answer_correctness']:.4f}\")\n",
    "            print(f\"      Saved: {filename}\")\n",
    "\n",
    "# PolQA OpenAI evaluations\n",
    "print(\"\\n### POLQA OPENAI DATASET ###\\n\")\n",
    "for get_retriever in polqa_openai_retriever_functions:\n",
    "    retriever, retriever_name = get_retriever()\n",
    "    print(f\"\\nRetriever: {retriever_name}\")\n",
    "    \n",
    "    print(f\"  Generator: gpt-4o-mini\")\n",
    "    generator = OpenAIGenerator(cache)\n",
    "    \n",
    "    for n in ns:\n",
    "        print(f\"    Evaluating with n={n}...\")\n",
    "        summary, detailed, manual_rows = evaluate_generator_ragas_v2(\n",
    "            retriever_name,\n",
    "            retriever,\n",
    "            \"gpt-4o-mini\",\n",
    "            generator,\n",
    "            polqa_dataset,\n",
    "            \"polqa_openai\",\n",
    "            n\n",
    "        )\n",
    "        \n",
    "        all_summaries.append(summary)\n",
    "        \n",
    "        for detail in detailed:\n",
    "            detail['retriever'] = retriever_name\n",
    "            detail['generator'] = 'gpt-4o-mini'\n",
    "            detail['dataset'] = 'polqa_openai'\n",
    "            detail['n'] = n\n",
    "        all_detailed_results.extend(detailed)\n",
    "        \n",
    "        # Save manual evaluation file\n",
    "        filename = create_safe_filename_v2(retriever_name, \"gpt-4o-mini\", \"INST\", \"polqa_openai\", n)\n",
    "        filepath = f\"../../output/ragas_v2/manual_eval/{filename}\"\n",
    "        \n",
    "        with open(filepath, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            file.write(f\"# RETRIEVER: {clean_text_for_csv(retriever_name)}\\n\")\n",
    "            file.write(f\"# GENERATOR: gpt-4o-mini\\n\")\n",
    "            file.write(f\"# TYPE: INST\\n\")\n",
    "            file.write(f\"# DATASET: polqa_openai\\n\")\n",
    "            file.write(f\"# TOP_N: {n}\\n\")\n",
    "            file.write(f\"# CACHE_VERSION: openai\\n\")\n",
    "            file.write(f\"# RAGAS_VERSION: v2\\n\")\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "            writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "            writer.writerow([\"question\", \"question_id\", \"hasCorrectPassages\", \"ragas_v2_score\", \"faithfulness\", \"answer_relevance\", \"answer_correctness\", \"context_recall\", \"answer\", \"correct_answer\", \"manual_result\"])\n",
    "            writer.writerows(manual_rows)\n",
    "        \n",
    "        print(f\"      RAGAS V2: {summary['ragas_v2']:.4f} | Correctness: {summary['answer_correctness']:.4f}\")\n",
    "        print(f\"      Saved: {filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(f\"Total configurations evaluated: {len(all_summaries)}\")\n",
    "print(f\"Manual evaluation files saved to: ../../output/ragas_v2/manual_eval/\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c70a4a",
   "metadata": {},
   "source": [
    "## Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary results\n",
    "summary_file = \"../../output/ragas_v2/ragas_v2_summary.csv\"\n",
    "with open(summary_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        'dataset', 'retriever', 'generator', 'n', \n",
    "        'ragas_v2', 'faithfulness', 'answer_relevance', \n",
    "        'answer_correctness', 'context_recall', 'num_questions'\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_summaries)\n",
    "\n",
    "print(f\"✅ Summary results saved to: {summary_file}\")\n",
    "\n",
    "# Save detailed results\n",
    "detailed_file = \"../../output/ragas_v2/ragas_v2_detailed.csv\"\n",
    "with open(detailed_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    if all_detailed_results:\n",
    "        writer = csv.DictWriter(file, fieldnames=all_detailed_results[0].keys(), quoting=csv.QUOTE_ALL)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_detailed_results)\n",
    "\n",
    "print(f\"✅ Detailed results saved to: {detailed_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3569dd",
   "metadata": {},
   "source": [
    "## Analysis: Top and Bottom Performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load summary results\n",
    "df = pd.DataFrame(all_summaries)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 CONFIGURATIONS BY RAGAS V2 SCORE\")\n",
    "print(\"=\" * 80)\n",
    "top_10 = df.nlargest(10, 'ragas_v2')[['dataset', 'generator', 'n', 'ragas_v2', 'answer_correctness']]\n",
    "for idx, row in top_10.iterrows():\n",
    "    print(f\"{row['dataset']:15} | {row['generator'][:30]:30} | n={row['n']} | RAGAS: {row['ragas_v2']:.4f} | Correctness: {row['answer_correctness']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BOTTOM 10 CONFIGURATIONS BY RAGAS V2 SCORE\")\n",
    "print(\"=\" * 80)\n",
    "bottom_10 = df.nsmallest(10, 'ragas_v2')[['dataset', 'generator', 'n', 'ragas_v2', 'answer_correctness']]\n",
    "for idx, row in bottom_10.iterrows():\n",
    "    print(f\"{row['dataset']:15} | {row['generator'][:30]:30} | n={row['n']} | RAGAS: {row['ragas_v2']:.4f} | Correctness: {row['answer_correctness']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AVERAGE SCORES BY GENERATOR\")\n",
    "print(\"=\" * 80)\n",
    "by_generator = df.groupby('generator')[['ragas_v2', 'answer_correctness', 'faithfulness', 'answer_relevance']].mean()\n",
    "print(by_generator.sort_values('ragas_v2', ascending=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AVERAGE SCORES BY DATASET\")\n",
    "print(\"=\" * 80)\n",
    "by_dataset = df.groupby('dataset')[['ragas_v2', 'answer_correctness', 'faithfulness', 'answer_relevance']].mean()\n",
    "print(by_dataset.sort_values('ragas_v2', ascending=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AVERAGE SCORES BY N\")\n",
    "print(\"=\" * 80)\n",
    "by_n = df.groupby('n')[['ragas_v2', 'answer_correctness', 'faithfulness', 'answer_relevance']].mean()\n",
    "print(by_n.sort_values('ragas_v2', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681e0c6",
   "metadata": {},
   "source": [
    "## Sample Answers for Manual Inspection\n",
    "\n",
    "Let's look at some actual answers to see if the reasoning problem is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few random examples\n",
    "import random\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE ANSWERS FROM RAGAS V2 EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample_results = random.sample(all_detailed_results, min(10, len(all_detailed_results)))\n",
    "\n",
    "for i, result in enumerate(sample_results, 1):\n",
    "    print(f\"\\n--- EXAMPLE {i} ---\")\n",
    "    print(f\"Dataset: {result['dataset']}\")\n",
    "    print(f\"Generator: {result['generator'][:40]}\")\n",
    "    print(f\"N: {result['n']}\")\n",
    "    print(f\"\\nQuestion: {result['question']}\")\n",
    "    print(f\"\\nGenerated Answer: {result['answer']}\")\n",
    "    print(f\"\\nCorrect Answer(s): {result['correct_answers']}\")\n",
    "    print(f\"\\nRAGAS V2: {result['ragas_v2']:.4f}\")\n",
    "    print(f\"Correctness: {result['answer_correctness']:.4f}\")\n",
    "    print(f\"Faithfulness: {result['faithfulness']:.4f}\")\n",
    "    print(f\"Relevance: {result['answer_relevance']:.4f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff45bf3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook:\n",
    "1. ✅ **Regenerated ALL answers** using improved prompt with cache key `instruction_v3` (NOT using old cache)\n",
    "2. ✅ **Evaluated with RAGAS V2** including answer correctness compared to ground truth\n",
    "3. ✅ **Saved 56 manual evaluation files** (28 PoQuAD + 28 PolQA) with RAGAS V2 scores in CSV format\n",
    "4. ✅ **Saved summary and detailed results** to CSV files\n",
    "5. ✅ **Provided comprehensive analysis** of performance\n",
    "\n",
    "### Output Files:\n",
    "\n",
    "**Summary Results:**\n",
    "- `../../output/ragas_v2/ragas_v2_summary.csv` - Aggregate scores per configuration\n",
    "\n",
    "**Detailed Results:**\n",
    "- `../../output/ragas_v2/ragas_v2_detailed.csv` - Per-question results with all metrics\n",
    "\n",
    "**Manual Evaluation Files (56 files):**\n",
    "- `../../output/ragas_v2/manual_eval/ragas_v2_*.csv` - One file per configuration\n",
    "- Each file contains: question, question_id, hasCorrectPassages, answer, correct_answer, ragas_v2_score, manual_result\n",
    "- Files include metadata headers showing retriever, generator, dataset, n, and versions\n",
    "\n",
    "### Key Improvements:\n",
    "\n",
    "1. **New Answers**: All generated with improved prompt (instruction_v3) that emphasizes \"ONLY answer, NO reasoning\"\n",
    "2. **Better Evaluation**: RAGAS V2 with continuous scoring and ground truth comparison\n",
    "3. **Ready for Manual Review**: CSV files formatted for Excel with RAGAS V2 scores for comparison\n",
    "\n",
    "Check the sample answers above to verify reasoning is removed, and use the manual evaluation files to compare your manual scores with RAGAS V2 automated scores!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
