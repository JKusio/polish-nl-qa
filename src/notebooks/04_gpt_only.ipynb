{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7542c37",
   "metadata": {},
   "source": [
    "# GPT-4o-mini RAGAS V2 Evaluation - ONLY GPT\n",
    "\n",
    "This notebook evaluates **ONLY GPT-4o-mini** using RAGAS V2 metrics.\n",
    "\n",
    "## Steps:\n",
    "1. Load GPT answers from JSONL files to cache\n",
    "2. Evaluate GPT-4o-mini with RAGAS V2\n",
    "3. Generate manual_eval CSV files\n",
    "\n",
    "**NO INSTRUCTION MODELS - JUST GPT!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb4f9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bb33eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v7/y_kh76zd2s55cwm9bpl4cg500000gn/T/ipykernel_63378/2018490365.py:5: UserWarning: Qdrant client version 1.14.3 is incompatible with server version 1.3.1. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from qdrant_client import QdrantClient\n",
    "from cache.cache import Cache\n",
    "\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "es_client = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"],\n",
    ")\n",
    "cache = Cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f1d70e",
   "metadata": {},
   "source": [
    "## Load GPT Answers to Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f97502cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded gpt-4o-mini_clarin-pl-poquad-1000.jsonl\n",
      "✓ Loaded gpt-4o-mini_clarin-pl-poquad-100000.jsonl\n",
      "✓ Loaded gpt-4o-mini_clarin-pl-poquad-2000.jsonl\n",
      "✓ Loaded gpt-4o-mini_clarin-pl-poquad-500.jsonl\n",
      "✓ Loaded gpt-4o-mini_ipipan-polqa-1000.jsonl\n",
      "✓ Loaded gpt-4o-mini_ipipan-polqa-100000.jsonl\n",
      "✓ Loaded gpt-4o-mini_ipipan-polqa-2000.jsonl\n",
      "✓ Loaded gpt-4o-mini_ipipan-polqa-500.jsonl\n",
      "\n",
      "✅ Loaded 1898 GPT answers to cache\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from common.utils import replace_slash_with_dash\n",
    "\n",
    "def load_openai_answers_to_cache(cache):\n",
    "    \"\"\"Load GPT-4o-mini answers from batch files to cache\"\"\"\n",
    "    datasets = [\n",
    "        \"clarin-pl-poquad-1000\",\n",
    "        \"clarin-pl-poquad-100000\", \n",
    "        \"clarin-pl-poquad-2000\",\n",
    "        \"clarin-pl-poquad-500\",\n",
    "        \"ipipan-polqa-1000\",\n",
    "        \"ipipan-polqa-100000\",\n",
    "        \"ipipan-polqa-2000\", \n",
    "        \"ipipan-polqa-500\"\n",
    "    ]\n",
    "    \n",
    "    loaded_count = 0\n",
    "    for dataset_key in datasets:\n",
    "        filename = replace_slash_with_dash(f\"gpt-4o-mini_{dataset_key}.jsonl\")\n",
    "        filepath = f\"../../openai_batches/{filename}\"\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    data = json.loads(line)\n",
    "                    custom_id = data[\"custom_id\"]\n",
    "                    answer = data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "                    cache.set(custom_id, answer)\n",
    "                    loaded_count += 1\n",
    "            print(f\"✓ Loaded {filename}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ File not found: {filename}\")\n",
    "    \n",
    "    print(f\"\\n✅ Loaded {loaded_count} GPT answers to cache\")\n",
    "\n",
    "load_openai_answers_to_cache(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df64d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 batch output files\n",
      "Found 7 curated input files\n",
      "Loaded 1930 unique hashes from curated input files\n",
      "✓ Loaded batch_68eeb9f42f288190bfe3ae2a4fe66c4c_output.jsonl: 433 answers\n",
      "✓ Loaded batch_68eeb9dd63008190b27d1ddb326c4b1b_output.jsonl: 126 answers\n",
      "✓ Loaded batch_68eeba02bdb8819087a65b0d57c86e99_output.jsonl: 442 answers\n",
      "✓ Loaded batch_68eeba269b108190be495cb28e762328_output.jsonl: 171 answers\n",
      "✓ Loaded batch_68eeb9e74b34819097b4a5740168a11a_output.jsonl: 8 answers\n",
      "✓ Loaded batch_68eeb9cb29c08190b8584366091d9ffa_output.jsonl: 410 answers\n",
      "✓ Loaded batch_68eeba0fe5f08190b0a9a7c0984603f8_output.jsonl: 340 answers\n",
      "\n",
      "✅ Loaded 1930 curated GPT answers to cache\n"
     ]
    }
   ],
   "source": [
    "# Load CURATED batch results from OpenAI\n",
    "import os\n",
    "\n",
    "curated_loaded = 0\n",
    "batch_files = []\n",
    "\n",
    "# Find all batch output files\n",
    "for filename in os.listdir(\"../openai_batches\"):\n",
    "    if filename.startswith(\"batch_\") and filename.endswith(\"_output.jsonl\"):\n",
    "        batch_files.append(os.path.join(\"../openai_batches\", filename))\n",
    "\n",
    "print(f\"Found {len(batch_files)} batch output files\")\n",
    "\n",
    "# Load curated input files to map hashes\n",
    "curated_input_files = []\n",
    "for filename in os.listdir(\"../openai_batches\"):\n",
    "    if \"curated\" in filename and filename.startswith(\"gpt-4o-mini_\") and not filename.endswith(\"_output.jsonl\"):\n",
    "        curated_input_files.append(os.path.join(\"../openai_batches\", filename))\n",
    "\n",
    "print(f\"Found {len(curated_input_files)} curated input files\")\n",
    "\n",
    "# Build hash -> input file mapping\n",
    "hash_to_input = {}\n",
    "for input_file in curated_input_files:\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            custom_id = data[\"custom_id\"]\n",
    "            hash_to_input[custom_id] = os.path.basename(input_file)\n",
    "\n",
    "print(f\"Loaded {len(hash_to_input)} unique hashes from curated input files\")\n",
    "\n",
    "# Load batch outputs to cache\n",
    "for batch_file in batch_files:\n",
    "    # Check first line to see if it's a curated batch\n",
    "    with open(batch_file, 'r', encoding='utf-8') as f:\n",
    "        first_line = f.readline()\n",
    "        first_data = json.loads(first_line)\n",
    "        sample_hash = first_data[\"custom_id\"]\n",
    "    \n",
    "    if sample_hash in hash_to_input:\n",
    "        # This is a curated batch - load it\n",
    "        count = 0\n",
    "        with open(batch_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                custom_id = data[\"custom_id\"]\n",
    "                try:\n",
    "                    answer = data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "                    cache.set(custom_id, answer)\n",
    "                    count += 1\n",
    "                    curated_loaded += 1\n",
    "                except (KeyError, IndexError):\n",
    "                    pass\n",
    "        print(f\"✓ Loaded {os.path.basename(batch_file)}: {count} answers\")\n",
    "\n",
    "print(f\"\\n✅ Loaded {curated_loaded} curated GPT answers to cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e9c400",
   "metadata": {},
   "source": [
    "## Load Datasets and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c06120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using CURATED datasets (manually selected questions)\n",
      "PoQuAD dataset: 100 questions\n",
      "PolQA dataset: 100 questions\n"
     ]
    }
   ],
   "source": [
    "from common.names import OPENAI_EMBEDDING_MODEL_NAMES\n",
    "from repository.qdrant_openai_repository import QdrantOpenAIRepository\n",
    "from retrievers.qdrant_retriever import QdrantRetriever\n",
    "from generators.openai_generator import OpenAIGenerator\n",
    "from qdrant_client.models import Distance\n",
    "from dataset.curated_dataset_getter import CuratedDatasetGetter\n",
    "\n",
    "# Load CURATED datasets (for which we just generated GPT answers)\n",
    "poquad_dataset = CuratedDatasetGetter.get_curated_poquad()\n",
    "polqa_dataset = CuratedDatasetGetter.get_curated_polqa()\n",
    "\n",
    "print(f\"✅ Using CURATED datasets (manually selected questions)\")\n",
    "print(f\"PoQuAD dataset: {len(poquad_dataset)} questions\")\n",
    "print(f\"PolQA dataset: {len(polqa_dataset)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4dc95",
   "metadata": {},
   "source": [
    "## Define OpenAI Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a7ab4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_poquad_openai_retriever():\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.EUCLID, cache\n",
    "    )\n",
    "    retriever = QdrantRetriever(repository, \"clarin-pl-poquad-2000\")\n",
    "    return (retriever, \"text-embedding-3-large-Euclid-clarin-pl-poquad-2000\")\n",
    "\n",
    "def get_worst_poquad_openai_retriever():\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.COSINE, cache\n",
    "    )\n",
    "    retriever = QdrantRetriever(repository, \"clarin-pl-poquad-500\")\n",
    "    return (retriever, \"text-embedding-3-large-Cosine-clarin-pl-poquad-500\")\n",
    "\n",
    "def get_best_polqa_openai_retriever():\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.EUCLID, cache\n",
    "    )\n",
    "    retriever = QdrantRetriever(repository, \"ipipan-polqa-2000\")\n",
    "    return (retriever, \"text-embedding-3-large-Euclid-ipipan-polqa-2000\")\n",
    "\n",
    "def get_worst_polqa_openai_retriever():\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.COSINE, cache\n",
    "    )\n",
    "    retriever = QdrantRetriever(repository, \"ipipan-polqa-500\")\n",
    "    return (retriever, \"text-embedding-3-large-Cosine-ipipan-polqa-500\")\n",
    "\n",
    "poquad_openai_retriever_functions = [\n",
    "    get_best_poquad_openai_retriever,\n",
    "    get_worst_poquad_openai_retriever,\n",
    "]\n",
    "\n",
    "polqa_openai_retriever_functions = [\n",
    "    get_best_polqa_openai_retriever,\n",
    "    get_worst_polqa_openai_retriever,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4028ad7",
   "metadata": {},
   "source": [
    "## Initialize RAGAS V2 Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb56a4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "✅ RAGAS V2 evaluator initialized!\n"
     ]
    }
   ],
   "source": [
    "from evaluation.ragas_evaulator_v2 import RAGASEvaluatorV2\n",
    "from vectorizer.hf_vectorizer import HFVectorizer\n",
    "from common.names import INST_MODEL_PATHS\n",
    "\n",
    "vectorizer = HFVectorizer(\"intfloat/multilingual-e5-large\", cache)\n",
    "ragas_evaluator = RAGASEvaluatorV2(\n",
    "    reranker_model_name=\"sdadas/polish-reranker-large-ranknet\",\n",
    "    cache=cache,\n",
    "    generator_model_name=INST_MODEL_PATHS[2],  # PLLuM-12B for question generation\n",
    "    vectorizer=vectorizer,\n",
    ")\n",
    "\n",
    "print(\"✅ RAGAS V2 evaluator initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d53e180",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf6dad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_text_for_csv(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    cleaned = str(text).replace('\\n', ' ').replace('\\r', ' ')\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    return cleaned\n",
    "\n",
    "def evaluate_gpt_ragas_v2(retriever_name, retriever, dataset, dataset_name, n):\n",
    "    results = []\n",
    "    manual_eval_rows = []\n",
    "    generator = OpenAIGenerator(cache)\n",
    "    \n",
    "    for i, entry in enumerate(tqdm(dataset, desc=f\"{dataset_name} - GPT-4o-mini\")):\n",
    "        question = entry.question\n",
    "        correct_passage_id = entry.passage_id\n",
    "        correct_answers = entry.answers\n",
    "        \n",
    "        # Get retrieval results\n",
    "        retriever_result = retriever.get_relevant_passages(question)\n",
    "        passages = [passage for (passage, _) in retriever_result.passages]\n",
    "        top_n_passages = passages[:n]\n",
    "        \n",
    "        # Generate answer from cache\n",
    "        answer = generator.generate_answer(question, top_n_passages)\n",
    "        \n",
    "        # Check if correct passage in top n\n",
    "        retrieved_ids = [passage.id for passage in top_n_passages]\n",
    "        has_correct_passages = str(correct_passage_id in retrieved_ids).upper()\n",
    "        \n",
    "        # RAGAS V2 evaluation\n",
    "        ragas_score = ragas_evaluator.ragas(\n",
    "            retriever_result,\n",
    "            correct_passage_id,\n",
    "            answer,\n",
    "            correct_answers=correct_answers\n",
    "        )\n",
    "        \n",
    "        faithfulness = ragas_evaluator.faithfulness(retriever_result, answer)\n",
    "        answer_relevance = ragas_evaluator.answer_relevance(question, answer)\n",
    "        answer_correctness = ragas_evaluator.answer_correctness(answer, correct_answers)\n",
    "        context_recall = ragas_evaluator.context_recall(retriever_result, correct_passage_id)\n",
    "        \n",
    "        results.append({\n",
    "            'ragas_v2': ragas_score,\n",
    "            'faithfulness': faithfulness,\n",
    "            'answer_relevance': answer_relevance,\n",
    "            'answer_correctness': answer_correctness,\n",
    "            'context_recall': context_recall,\n",
    "        })\n",
    "        \n",
    "        # Manual eval row\n",
    "        question_id = f\"{dataset_name}_q{i+1}\"\n",
    "        question_text_clean = clean_text_for_csv(question)\n",
    "        answer_clean = clean_text_for_csv(answer)\n",
    "        \n",
    "        if isinstance(correct_answers, list):\n",
    "            correct_answer_text = \" | \".join([clean_text_for_csv(ans) for ans in correct_answers])\n",
    "        else:\n",
    "            correct_answer_text = clean_text_for_csv(str(correct_answers))\n",
    "        \n",
    "        manual_eval_rows.append([\n",
    "            question_text_clean,\n",
    "            question_id,\n",
    "            has_correct_passages,\n",
    "            f\"{ragas_score:.4f}\",\n",
    "            f\"{faithfulness:.4f}\",\n",
    "            f\"{answer_relevance:.4f}\",\n",
    "            f\"{answer_correctness:.4f}\",\n",
    "            f\"{context_recall:.4f}\",\n",
    "            answer_clean,\n",
    "            correct_answer_text,\n",
    "            \"\"\n",
    "        ])\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_ragas = sum(r['ragas_v2'] for r in results) / len(results)\n",
    "    avg_faithfulness = sum(r['faithfulness'] for r in results) / len(results)\n",
    "    avg_relevance = sum(r['answer_relevance'] for r in results) / len(results)\n",
    "    avg_correctness = sum(r['answer_correctness'] for r in results) / len(results)\n",
    "    avg_recall = sum(r['context_recall'] for r in results) / len(results)\n",
    "    \n",
    "    summary = {\n",
    "        'retriever': retriever_name,\n",
    "        'dataset': dataset_name,\n",
    "        'n': n,\n",
    "        'ragas_v2': avg_ragas,\n",
    "        'faithfulness': avg_faithfulness,\n",
    "        'answer_relevance': avg_relevance,\n",
    "        'answer_correctness': avg_correctness,\n",
    "        'context_recall': avg_recall,\n",
    "    }\n",
    "    \n",
    "    return summary, manual_eval_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a7f48",
   "metadata": {},
   "source": [
    "## Run GPT Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a10ad669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GPT-4o-mini RAGAS V2 EVALUATION\n",
      "================================================================================\n",
      "\n",
      "### POQUAD OPENAI ###\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "\n",
      "Retriever: text-embedding-3-large-Euclid-clarin-pl-poquad-2000\n",
      "  Evaluating n=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "poquad_openai - GPT-4o-mini: 100%|██████████| 100/100 [00:01<00:00, 72.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RAGAS V2: 0.8092 | Correctness: 0.6114\n",
      "    Saved: ragas_v2_poquad_openai_gpt_4o_mini_n1_text-embedding-3-large-Euclid-clarin-pl-poquad-2000.csv\n",
      "  Evaluating n=5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "poquad_openai - GPT-4o-mini: 100%|██████████| 100/100 [03:12<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RAGAS V2: 0.8136 | Correctness: 0.6221\n",
      "    Saved: ragas_v2_poquad_openai_gpt_4o_mini_n5_text-embedding-3-large-Euclid-clarin-pl-poquad-2000.csv\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "\n",
      "Retriever: text-embedding-3-large-Cosine-clarin-pl-poquad-500\n",
      "  Evaluating n=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "poquad_openai - GPT-4o-mini: 100%|██████████| 100/100 [04:38<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RAGAS V2: 0.8033 | Correctness: 0.6091\n",
      "    Saved: ragas_v2_poquad_openai_gpt_4o_mini_n1_text-embedding-3-large-Cosine-clarin-pl-poquad-500.csv\n",
      "  Evaluating n=5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "poquad_openai - GPT-4o-mini: 100%|██████████| 100/100 [04:06<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RAGAS V2: 0.8069 | Correctness: 0.6181\n",
      "    Saved: ragas_v2_poquad_openai_gpt_4o_mini_n5_text-embedding-3-large-Cosine-clarin-pl-poquad-500.csv\n",
      "\n",
      "### POLQA OPENAI ###\n",
      "\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "\n",
      "Retriever: text-embedding-3-large-Euclid-ipipan-polqa-2000\n",
      "  Evaluating n=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polqa_openai - GPT-4o-mini: 100%|██████████| 100/100 [07:04<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RAGAS V2: 0.8100 | Correctness: 0.6126\n",
      "    Saved: ragas_v2_polqa_openai_gpt_4o_mini_n1_text-embedding-3-large-Euclid-ipipan-polqa-2000.csv\n",
      "  Evaluating n=5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polqa_openai - GPT-4o-mini: 100%|██████████| 100/100 [04:20<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RAGAS V2: 0.8136 | Correctness: 0.6102\n",
      "    Saved: ragas_v2_polqa_openai_gpt_4o_mini_n5_text-embedding-3-large-Euclid-ipipan-polqa-2000.csv\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "\n",
      "Retriever: text-embedding-3-large-Cosine-ipipan-polqa-500\n",
      "  Evaluating n=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polqa_openai - GPT-4o-mini: 100%|██████████| 100/100 [01:02<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RAGAS V2: 0.8090 | Correctness: 0.6136\n",
      "    Saved: ragas_v2_polqa_openai_gpt_4o_mini_n1_text-embedding-3-large-Cosine-ipipan-polqa-500.csv\n",
      "  Evaluating n=5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "polqa_openai - GPT-4o-mini: 100%|██████████| 100/100 [01:20<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RAGAS V2: 0.8128 | Correctness: 0.6098\n",
      "    Saved: ragas_v2_polqa_openai_gpt_4o_mini_n5_text-embedding-3-large-Cosine-ipipan-polqa-500.csv\n",
      "\n",
      "================================================================================\n",
      "✅ GPT EVALUATION COMPLETE!\n",
      "Generated 8 manual evaluation files\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"../../output/ragas_v2\", exist_ok=True)\n",
    "os.makedirs(\"../../output/ragas_v2/manual_eval\", exist_ok=True)\n",
    "\n",
    "all_summaries = []\n",
    "ns = [1, 5]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GPT-4o-mini RAGAS V2 EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# PoQuAD OpenAI\n",
    "print(\"\\n### POQUAD OPENAI ###\\n\")\n",
    "for get_retriever in poquad_openai_retriever_functions:\n",
    "    retriever, retriever_name = get_retriever()\n",
    "    print(f\"\\nRetriever: {retriever_name}\")\n",
    "    \n",
    "    for n in ns:\n",
    "        print(f\"  Evaluating n={n}...\")\n",
    "        summary, manual_rows = evaluate_gpt_ragas_v2(\n",
    "            retriever_name,\n",
    "            retriever,\n",
    "            poquad_dataset,\n",
    "            \"poquad_openai\",\n",
    "            n\n",
    "        )\n",
    "        \n",
    "        all_summaries.append(summary)\n",
    "        \n",
    "        # Save manual eval file\n",
    "        filename = f\"ragas_v2_poquad_openai_gpt_4o_mini_n{n}_{retriever_name.replace('/', '_')}.csv\"\n",
    "        filepath = f\"../../output/ragas_v2/manual_eval/{filename}\"\n",
    "        \n",
    "        with open(filepath, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            file.write(f\"# RETRIEVER: {retriever_name}\\n\")\n",
    "            file.write(f\"# GENERATOR: gpt-4o-mini\\n\")\n",
    "            file.write(f\"# DATASET: poquad_openai\\n\")\n",
    "            file.write(f\"# TOP_N: {n}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "            writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "            writer.writerow([\"question\", \"question_id\", \"hasCorrectPassages\", \"ragas_v2_score\", \n",
    "                           \"faithfulness\", \"answer_relevance\", \"answer_correctness\", \"context_recall\", \n",
    "                           \"answer\", \"correct_answer\", \"manual_result\"])\n",
    "            writer.writerows(manual_rows)\n",
    "        \n",
    "        print(f\"    RAGAS V2: {summary['ragas_v2']:.4f} | Correctness: {summary['answer_correctness']:.4f}\")\n",
    "        print(f\"    Saved: {filename}\")\n",
    "\n",
    "# PolQA OpenAI\n",
    "print(\"\\n### POLQA OPENAI ###\\n\")\n",
    "for get_retriever in polqa_openai_retriever_functions:\n",
    "    retriever, retriever_name = get_retriever()\n",
    "    print(f\"\\nRetriever: {retriever_name}\")\n",
    "    \n",
    "    for n in ns:\n",
    "        print(f\"  Evaluating n={n}...\")\n",
    "        summary, manual_rows = evaluate_gpt_ragas_v2(\n",
    "            retriever_name,\n",
    "            retriever,\n",
    "            polqa_dataset,\n",
    "            \"polqa_openai\",\n",
    "            n\n",
    "        )\n",
    "        \n",
    "        all_summaries.append(summary)\n",
    "        \n",
    "        # Save manual eval file\n",
    "        filename = f\"ragas_v2_polqa_openai_gpt_4o_mini_n{n}_{retriever_name.replace('/', '_')}.csv\"\n",
    "        filepath = f\"../../output/ragas_v2/manual_eval/{filename}\"\n",
    "        \n",
    "        with open(filepath, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            file.write(f\"# RETRIEVER: {retriever_name}\\n\")\n",
    "            file.write(f\"# GENERATOR: gpt-4o-mini\\n\")\n",
    "            file.write(f\"# DATASET: polqa_openai\\n\")\n",
    "            file.write(f\"# TOP_N: {n}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "            writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "            writer.writerow([\"question\", \"question_id\", \"hasCorrectPassages\", \"ragas_v2_score\", \n",
    "                           \"faithfulness\", \"answer_relevance\", \"answer_correctness\", \"context_recall\", \n",
    "                           \"answer\", \"correct_answer\", \"manual_result\"])\n",
    "            writer.writerows(manual_rows)\n",
    "        \n",
    "        print(f\"    RAGAS V2: {summary['ragas_v2']:.4f} | Correctness: {summary['answer_correctness']:.4f}\")\n",
    "        print(f\"    Saved: {filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ GPT EVALUATION COMPLETE!\")\n",
    "print(f\"Generated {len(all_summaries)} manual evaluation files\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e7b28",
   "metadata": {},
   "source": [
    "## Summary Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e84d0584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-4o-mini RAGAS V2 Results:\n",
      "================================================================================\n",
      "poquad_openai        | n=1 | RAGAS: 0.8092 | Correctness: 0.6114\n",
      "poquad_openai        | n=5 | RAGAS: 0.8136 | Correctness: 0.6221\n",
      "poquad_openai        | n=1 | RAGAS: 0.8033 | Correctness: 0.6091\n",
      "poquad_openai        | n=5 | RAGAS: 0.8069 | Correctness: 0.6181\n",
      "polqa_openai         | n=1 | RAGAS: 0.8100 | Correctness: 0.6126\n",
      "polqa_openai         | n=5 | RAGAS: 0.8136 | Correctness: 0.6102\n",
      "polqa_openai         | n=1 | RAGAS: 0.8090 | Correctness: 0.6136\n",
      "polqa_openai         | n=5 | RAGAS: 0.8128 | Correctness: 0.6098\n",
      "\n",
      "================================================================================\n",
      "Average by N:\n",
      "   ragas_v2  answer_correctness\n",
      "n                              \n",
      "1  0.807898            0.611684\n",
      "5  0.811747            0.615045\n",
      "\n",
      "================================================================================\n",
      "Average by Dataset:\n",
      "               ragas_v2  answer_correctness\n",
      "dataset                                    \n",
      "polqa_openai   0.811355            0.611550\n",
      "poquad_openai  0.808291            0.615178\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_summaries)\n",
    "print(\"\\nGPT-4o-mini RAGAS V2 Results:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"{row['dataset']:20} | n={row['n']} | RAGAS: {row['ragas_v2']:.4f} | Correctness: {row['answer_correctness']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Average by N:\")\n",
    "print(df.groupby('n')[['ragas_v2', 'answer_correctness']].mean())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Average by Dataset:\")\n",
    "print(df.groupby('dataset')[['ragas_v2', 'answer_correctness']].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
