{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a9009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533bd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from qdrant_client import QdrantClient\n",
    "from cache.cache import Cache\n",
    "\n",
    "\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "es_client = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"],\n",
    ")\n",
    "cache = Cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5fde4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.names import (\n",
    "    OPENAI_EMBEDDING_MODEL_NAMES,\n",
    "    PASSAGE_PREFIX_MAP,\n",
    "    QUERY_PREFIX_MAP,\n",
    ")\n",
    "from repository.es_repository import ESRepository\n",
    "from repository.qdrant_openai_repository import QdrantOpenAIRepository\n",
    "from repository.qdrant_repository import QdrantRepository\n",
    "from qdrant_client.models import Distance\n",
    "\n",
    "from rerankers.hf_reranker import HFReranker\n",
    "from retrievers.es_retriever import ESRetriever\n",
    "from retrievers.hybrid_retriever import HybridRetriever\n",
    "from retrievers.qdrant_retriever import QdrantRetriever\n",
    "from retrievers.retriever import Retriever\n",
    "\n",
    "\n",
    "def get_best_poquad_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"clarin-pl-poquad-100000\"\n",
    "    es_index = \"morfologik_index\"\n",
    "    qdrant_model = \"intfloat/multilingual-e5-large\"\n",
    "    reranker_model = \"sdadas/polish-reranker-large-ranknet\"\n",
    "    alpha = 0.5\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "    passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "    query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "    qdrant_repository = QdrantRepository.get_repository(\n",
    "        qdrant_client,\n",
    "        qdrant_model,\n",
    "        Distance.COSINE,\n",
    "        cache,\n",
    "        passage_prefix,\n",
    "        query_prefix,\n",
    "    )\n",
    "    reranker = HFReranker(reranker_model, cache)\n",
    "\n",
    "    retriever = HybridRetriever(\n",
    "        es_repository, qdrant_repository, dataset_key, alpha, reranker\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"morfologik_index-intfloat/multilingual-e5-large-Cosine-clarin-pl-poquad-100000-0.5-sdadas/polish-reranker-large-ranknet\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_50p_poquad_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"clarin-pl-poquad-1000\"\n",
    "    qdrant_model = \"sdadas/mmlw-retrieval-roberta-large\"\n",
    "\n",
    "    passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "    query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "    qdrant_repository = QdrantRepository.get_repository(\n",
    "        qdrant_client,\n",
    "        qdrant_model,\n",
    "        Distance.EUCLID,\n",
    "        cache,\n",
    "        passage_prefix,\n",
    "        query_prefix,\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(qdrant_repository, dataset_key)\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"sdadas/mmlw-retrieval-roberta-large-Euclid-clarin-pl-poquad-1000\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_worst_poquad_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"clarin-pl-poquad-500\"\n",
    "    es_index = \"basic_index\"\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "\n",
    "    retriever = ESRetriever(es_repository, dataset_key)\n",
    "\n",
    "    return (retriever, \"basic_index-clarin-pl-poquad-500\")\n",
    "\n",
    "\n",
    "def get_best_poquad_openai_retriever() -> tuple[Retriever, str]:\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.COSINE, cache\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(repository, \"clarin-pl-poquad-2000\")\n",
    "\n",
    "    return (retriever, \"text-embedding-3-large-Cosine-clarin-pl-poquad-2000\")\n",
    "\n",
    "\n",
    "def get_worst_poquad_openai_retriever() -> tuple[Retriever, str]:\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.COSINE, cache\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(repository, \"clarin-pl-poquad-500\")\n",
    "\n",
    "    return (retriever, \"text-embedding-3-large-Cosine-clarin-pl-poquad-500\")\n",
    "\n",
    "\n",
    "def get_best_polqa_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"ipipan-polqa-1000\"\n",
    "    es_index = \"morfologik_index\"\n",
    "    qdrant_model = \"sdadas/mmlw-retrieval-roberta-large\"\n",
    "    reranker_model = \"sdadas/polish-reranker-large-ranknet\"\n",
    "    alpha = 0.75\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "    passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "    query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "    qdrant_repository = QdrantRepository.get_repository(\n",
    "        qdrant_client,\n",
    "        qdrant_model,\n",
    "        Distance.COSINE,\n",
    "        cache,\n",
    "        passage_prefix,\n",
    "        query_prefix,\n",
    "    )\n",
    "    reranker = HFReranker(reranker_model, cache)\n",
    "\n",
    "    retriever = HybridRetriever(\n",
    "        es_repository, qdrant_repository, dataset_key, alpha, reranker\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"morfologik_index-sdadas/mmlw-retrieval-roberta-large-Cosine-ipipan-polqa-1000-0.5-sdadas/polish-reranker-large-ranknet\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_50p_polqa_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"ipipan-polqa-1000\"\n",
    "    es_index = \"morfologik_index\"\n",
    "    qdrant_model = \"sdadas/mmlw-retrieval-roberta-large\"\n",
    "    alpha = 0.75\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "    passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "    query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "    qdrant_repository = QdrantRepository.get_repository(\n",
    "        qdrant_client,\n",
    "        qdrant_model,\n",
    "        Distance.COSINE,\n",
    "        cache,\n",
    "        passage_prefix,\n",
    "        query_prefix,\n",
    "    )\n",
    "\n",
    "    retriever = HybridRetriever(es_repository, qdrant_repository, dataset_key, alpha)\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"morfologik_index-sdadas/mmlw-retrieval-roberta-large-Cosine-ipipan-polqa-1000-0.75\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_worst_polqa_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"ipipan-polqa-500\"\n",
    "    es_index = \"basic_index\"\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "\n",
    "    retriever = ESRetriever(es_repository, dataset_key)\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"basic_index-ipipan-polqa-500\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_best_polqa_openai_retriever() -> tuple[Retriever, str]:\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.EUCLID, cache\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(repository, \"ipipan-polqa-2000\")\n",
    "\n",
    "    return (retriever, \"text-embedding-3-large-Euclid-ipipan-polqa-2000\")\n",
    "\n",
    "\n",
    "def get_worst_polqa_openai_retriever() -> tuple[Retriever, str]:\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.COSINE, cache\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(repository, \"ipipan-polqa-500\")\n",
    "\n",
    "    return (retriever, \"text-embedding-3-large-Cosine-ipipan-polqa-500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7929b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "poquad_retriever_functions = [\n",
    "    get_best_poquad_retriever,\n",
    "    get_50p_poquad_retriever,\n",
    "    get_worst_poquad_retriever,\n",
    "]\n",
    "\n",
    "poquad_openai_retriever_functions = [\n",
    "    get_best_poquad_openai_retriever,\n",
    "    get_worst_poquad_openai_retriever,\n",
    "]\n",
    "\n",
    "polqa_retriever_functions = [\n",
    "    get_best_polqa_retriever,\n",
    "    get_50p_polqa_retriever,\n",
    "    get_worst_polqa_retriever,\n",
    "]\n",
    "\n",
    "polqa_openai_retriever_functions = [\n",
    "    get_best_polqa_openai_retriever,\n",
    "    get_worst_polqa_openai_retriever,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbbc8957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.names import DATASET_SEED\n",
    "from dataset.polqa_dataset_getter import PolqaDatasetGetter\n",
    "from dataset.poquad_dataset_getter import PoquadDatasetGetter\n",
    "\n",
    "\n",
    "poquad_dataset_getter = PoquadDatasetGetter()\n",
    "polqa_dataset_getter = PolqaDatasetGetter()\n",
    "\n",
    "poquad_dataset = poquad_dataset_getter.get_random_n_test(500, DATASET_SEED)[:100]\n",
    "polqa_dataset = polqa_dataset_getter.get_random_n_test(500, DATASET_SEED)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2666c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33deb110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrievers_top_n(retriever_functions, dataset, ns):\n",
    "    results = {}\n",
    "    for get_retriever in retriever_functions:\n",
    "        retriever, retriever_name = get_retriever()\n",
    "        retriever_results = {}\n",
    "        for n in ns:\n",
    "            hits = []\n",
    "            for item in dataset:\n",
    "                question = item.question \n",
    "                correct_passage_id = item.passage_id \n",
    "                retriever_result = retriever.get_relevant_passages(question)\n",
    "                passages = [passage for (passage, _) in retriever_result.passages]\n",
    "                top_n_passages = passages[:n]\n",
    "                retrieved_ids = [passage.id for passage in top_n_passages]\n",
    "                hits.append(correct_passage_id in retrieved_ids)\n",
    "            retriever_results[n] = hits\n",
    "        results[retriever_name] = retriever_results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "825a7d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Cosine repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n"
     ]
    }
   ],
   "source": [
    "retriever_evaluation_results = {\n",
    "    \"poquad\": evaluate_retrievers_top_n(poquad_retriever_functions, poquad_dataset, ns),\n",
    "    \"poquad_openai\": evaluate_retrievers_top_n(poquad_openai_retriever_functions, poquad_dataset, ns),\n",
    "    \"polqa\": evaluate_retrievers_top_n(polqa_retriever_functions, polqa_dataset, ns),\n",
    "    \"polqa_openai\": evaluate_retrievers_top_n(polqa_openai_retriever_functions, polqa_dataset, ns),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34586353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results as csv\n",
    "import csv\n",
    "\n",
    "\n",
    "def save_retriever_scores_to_csv(results, filename):\n",
    "    with open(filename, mode=\"w\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"dataset\", \"retriever\", \"n\", \"hit_rate\"])\n",
    "        \n",
    "        for dataset_name, dataset_results in results.items():\n",
    "            for retriever_name, retriever_results in dataset_results.items():\n",
    "                for n, hits in retriever_results.items():\n",
    "                    hit_rate = sum(hits) / len(hits)\n",
    "                    writer.writerow(\n",
    "                        [\n",
    "                            dataset_name,\n",
    "                            retriever_name,\n",
    "                            str(n),\n",
    "                            str(hit_rate).replace(\".\", \",\"),\n",
    "                        ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f3dc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_retriever_scores_to_csv(retriever_evaluation_results, \"../../output/retriever_evaluation_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90c02b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_detailed_question_results(retriever_functions, dataset, ns, dataset_name):\n",
    "    \"\"\"Generate detailed results for each question showing which retrievers found the correct passage\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Create header with question info and all retriever-n combinations\n",
    "    header = [\"question_id\", \"question_text\", \"correct_passage_id\"]\n",
    "    retriever_columns = []\n",
    "    \n",
    "    for get_retriever in retriever_functions:\n",
    "        _, retriever_name = get_retriever()\n",
    "        for n in ns:\n",
    "            column_name = f\"{retriever_name}_n{n}\"\n",
    "            retriever_columns.append(column_name)\n",
    "            header.append(column_name)\n",
    "    \n",
    "    results.append(header)\n",
    "    \n",
    "    # Pre-load all retrievers once\n",
    "    retrievers = []\n",
    "    for get_retriever in retriever_functions:\n",
    "        print(f\"Loading retriever...\")\n",
    "        retriever, retriever_name = get_retriever()\n",
    "        retrievers.append((retriever, retriever_name))\n",
    "        print(f\"Loaded: {retriever_name}\")\n",
    "    \n",
    "    # Process each question\n",
    "    for i, item in enumerate(dataset):\n",
    "        question_id = f\"{dataset_name}_q{i+1}\"\n",
    "        question_text = item.question\n",
    "        correct_passage_id = item.passage_id\n",
    "        \n",
    "        row = [question_id, question_text, correct_passage_id]\n",
    "        \n",
    "        # Test each pre-loaded retriever configuration\n",
    "        for retriever, retriever_name in retrievers:\n",
    "            # Get retrieval results once for this question\n",
    "            retriever_result = retriever.get_relevant_passages(question_text)\n",
    "            passages = [passage for (passage, _) in retriever_result.passages]\n",
    "            \n",
    "            for n in ns:\n",
    "                top_n_passages = passages[:n]\n",
    "                retrieved_ids = [passage.id for passage in top_n_passages]\n",
    "                found = correct_passage_id in retrieved_ids\n",
    "                row.append(str(found).upper())\n",
    "        \n",
    "        results.append(row)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1} questions for {dataset_name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def save_detailed_results_to_csv(results, filename):\n",
    "    \"\"\"Save detailed question-level results to CSV\"\"\"\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in results:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e911a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating detailed question-level results...\n",
      "Processing PoQuAD dataset...\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# PoQuAD dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing PoQuAD dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m poquad_detailed \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_detailed_question_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoquad_retriever_functions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoquad_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoquad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m save_detailed_results_to_csv(poquad_detailed, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../output/poquad_detailed_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# PoQuAD OpenAI dataset  \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m, in \u001b[0;36mgenerate_detailed_question_results\u001b[0;34m(retriever_functions, dataset, ns, dataset_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m retriever_columns \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m get_retriever \u001b[38;5;129;01min\u001b[39;00m retriever_functions:\n\u001b[0;32m---> 10\u001b[0m     _, retriever_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m ns:\n\u001b[1;32m     12\u001b[0m         column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretriever_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_n\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36mget_best_poquad_retriever\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m query_prefix \u001b[38;5;241m=\u001b[39m QUERY_PREFIX_MAP[qdrant_model]\n\u001b[1;32m     28\u001b[0m qdrant_repository \u001b[38;5;241m=\u001b[39m QdrantRepository\u001b[38;5;241m.\u001b[39mget_repository(\n\u001b[1;32m     29\u001b[0m     qdrant_client,\n\u001b[1;32m     30\u001b[0m     qdrant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     query_prefix,\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 36\u001b[0m reranker \u001b[38;5;241m=\u001b[39m \u001b[43mHFReranker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreranker_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m retriever \u001b[38;5;241m=\u001b[39m HybridRetriever(\n\u001b[1;32m     39\u001b[0m     es_repository, qdrant_repository, dataset_key, alpha, reranker\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     retriever,\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmorfologik_index-intfloat/multilingual-e5-large-Cosine-clarin-pl-poquad-100000-0.5-sdadas/polish-reranker-large-ranknet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/src/notebooks/../rerankers/hf_reranker.py:16\u001b[0m, in \u001b[0;36mHFReranker.__init__\u001b[0;34m(self, model_name, cache)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m, cache: Cache):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mCrossEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRERANKER_MODEL_DIMENSIONS_MAP\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorizer with model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/sentence_transformers/cross_encoder/util.py:39\u001b[0m, in \u001b[0;36mcross_encoder_init_args_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier_dropout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m classifier_dropout\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:225\u001b[0m, in \u001b[0;36mCrossEncoder.__init__\u001b[0;34m(self, model_name_or_path, num_labels, max_length, activation_fn, device, cache_folder, trust_remote_code, revision, local_files_only, token, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    223\u001b[0m     device \u001b[38;5;241m=\u001b[39m get_device_name()\n\u001b[1;32m    224\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse pytorch device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Pass the model to the model card data for later use in generating a model card upon saving this model\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_card_data\u001b[38;5;241m.\u001b[39mregister_model(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/transformers/modeling_utils.py:4091\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   4087\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4088\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4089\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4090\u001b[0m         )\n\u001b[0;32m-> 4091\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate detailed results for each dataset\n",
    "print(\"Generating detailed question-level results...\")\n",
    "\n",
    "# PoQuAD dataset\n",
    "print(\"Processing PoQuAD dataset...\")\n",
    "poquad_detailed = generate_detailed_question_results(\n",
    "    poquad_retriever_functions, poquad_dataset, ns, \"poquad\"\n",
    ")\n",
    "save_detailed_results_to_csv(poquad_detailed, \"../../output/poquad_detailed_results.csv\")\n",
    "\n",
    "# PoQuAD OpenAI dataset  \n",
    "print(\"Processing PoQuAD OpenAI dataset...\")\n",
    "poquad_openai_detailed = generate_detailed_question_results(\n",
    "    poquad_openai_retriever_functions, poquad_dataset, ns, \"poquad_openai\"\n",
    ")\n",
    "save_detailed_results_to_csv(poquad_openai_detailed, \"../../output/poquad_openai_detailed_results.csv\")\n",
    "\n",
    "# PolQA dataset\n",
    "print(\"Processing PolQA dataset...\")\n",
    "polqa_detailed = generate_detailed_question_results(\n",
    "    polqa_retriever_functions, polqa_dataset, ns, \"polqa\"\n",
    ")\n",
    "save_detailed_results_to_csv(polqa_detailed, \"../../output/polqa_detailed_results.csv\")\n",
    "\n",
    "# PolQA OpenAI dataset\n",
    "print(\"Processing PolQA OpenAI dataset...\")\n",
    "polqa_openai_detailed = generate_detailed_question_results(\n",
    "    polqa_openai_retriever_functions, polqa_dataset, ns, \"polqa_openai\"\n",
    ")\n",
    "save_detailed_results_to_csv(polqa_openai_detailed, \"../../output/polqa_openai_detailed_results.csv\")\n",
    "\n",
    "print(\"All detailed results saved to CSV files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ccf894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the generators and model names from notebook 02\n",
    "from common.names import INST_MODEL_PATHS, QA_MODEL_NAMES\n",
    "from generators.instruction_generator import InstructionGenerator\n",
    "from generators.openai_generator import OpenAIGenerator\n",
    "from generators.question_answering_generator import QuestionAnsweringGenerator\n",
    "\n",
    "\n",
    "def generate_manual_evaluation_file(retriever, retriever_name, generator, generator_name, generator_type, dataset, dataset_name, n):\n",
    "    \"\"\"Generate a file for manual evaluation with questions, answers, and evaluation columns\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Add header with metadata\n",
    "    metadata_header = [f\"# RETRIEVER: {retriever_name}\"]\n",
    "    metadata_header.append(f\"# GENERATOR: {generator_name}\")\n",
    "    metadata_header.append(f\"# TYPE: {generator_type}\")\n",
    "    metadata_header.append(f\"# DATASET: {dataset_name}\")\n",
    "    metadata_header.append(f\"# TOP_N: {n}\")\n",
    "    metadata_header.append(\"\")  # Empty line\n",
    "    \n",
    "    # Add CSV header\n",
    "    header = [\"question\", \"question_id\", \"hasCorrectPassages\", \"answer\", \"correct_answer\", \"result\"]\n",
    "    \n",
    "    # Process each question\n",
    "    for i, item in enumerate(dataset):\n",
    "        question_id = f\"{dataset_name}_q{i+1}\"\n",
    "        question_text = item.question\n",
    "        correct_passage_id = item.passage_id\n",
    "        correct_answers = item.answers  # Get the correct answers from the dataset\n",
    "        \n",
    "        # Get retrieval results\n",
    "        retriever_result = retriever.get_relevant_passages(question_text)\n",
    "        passages = [passage for (passage, _) in retriever_result.passages]\n",
    "        top_n_passages = passages[:n]\n",
    "        \n",
    "        # Check if correct passage is retrieved\n",
    "        retrieved_ids = [passage.id for passage in top_n_passages]\n",
    "        has_correct_passages = str(correct_passage_id in retrieved_ids).upper()\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = generator.generate_answer(question_text, top_n_passages)\n",
    "        \n",
    "        # Format correct answers (join multiple answers with \" | \" if there are multiple)\n",
    "        if isinstance(correct_answers, list):\n",
    "            correct_answer_text = \" | \".join(correct_answers)\n",
    "        else:\n",
    "            correct_answer_text = str(correct_answers)\n",
    "        \n",
    "        # Create row\n",
    "        row = [question_text, question_id, has_correct_passages, answer, correct_answer_text, \"\"]  # Empty result column for manual evaluation\n",
    "        results.append(row)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1} questions for {generator_name} on {dataset_name}\")\n",
    "    \n",
    "    return metadata_header, header, results\n",
    "\n",
    "\n",
    "def save_manual_evaluation_file(metadata, header, results, filename):\n",
    "    \"\"\"Save manual evaluation file with metadata and CSV data\"\"\"\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        # Write metadata as comments\n",
    "        for line in metadata:\n",
    "            file.write(line + \"\\n\")\n",
    "        \n",
    "        # Write CSV data\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "        for row in results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def create_safe_filename(retriever_name, generator_name, generator_type, dataset_name, n):\n",
    "    \"\"\"Create a safe filename from the configuration\"\"\"\n",
    "    # Clean names for filename\n",
    "    safe_retriever = retriever_name.replace(\"/\", \"_\").replace(\"-\", \"_\")[:50]  # Limit length\n",
    "    safe_generator = generator_name.replace(\"/\", \"_\").replace(\"-\", \"_\")[:30]\n",
    "    \n",
    "    return f\"manual_eval_{dataset_name}_{safe_retriever}_{safe_generator}_{generator_type}_n{n}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c21b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for manual evaluation files\n",
    "import os\n",
    "os.makedirs(\"../../output/manual_eval\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7932ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating manual evaluation files for all combinations...\n",
      "\n",
      "=== Processing PoQuAD dataset ===\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Processing retriever: morfologik_index-intfloat/multilingual-e5-large-Cosine-clarin-pl-poquad-100000-0.5-sdadas/polish-reranker-large-ranknet\n",
      "  Generating with QA model: radlab/polish-qa-v2\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Processing retriever: morfologik_index-intfloat/multilingual-e5-large-Cosine-clarin-pl-poquad-100000-0.5-sdadas/polish-reranker-large-ranknet\n",
      "  Generating with QA model: radlab/polish-qa-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 20 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 30 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 40 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 50 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 60 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 70 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 80 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 90 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 100 questions for radlab/polish-qa-v2 on poquad\n",
      "  Generating with QA model: timpal0l/mdeberta-v3-base-squad2\n",
      "Processed 80 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 90 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 100 questions for radlab/polish-qa-v2 on poquad\n",
      "  Generating with QA model: timpal0l/mdeberta-v3-base-squad2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 20 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 30 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 40 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 50 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 60 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 70 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 80 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 90 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 100 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "  Generating with Instruction model: ../../models/Bielik-11B-v2.2-Instruct-q4\n",
      "Processed 10 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 10 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/Mistral-7B-Instruct-v0.2-q4\n",
      "Processed 80 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/Mistral-7B-Instruct-v0.2-q4\n",
      "Processed 10 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 20 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 30 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 40 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 50 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 60 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 70 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 10 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 20 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 30 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 40 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 50 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 60 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 70 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 80 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 90 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 100 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/PLLuM-12-B-instruct-q4\n",
      "Processed 80 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 90 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 100 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/PLLuM-12-B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/PLLuM-8B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/PLLuM-8B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 10 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Processing retriever: sdadas/mmlw-retrieval-roberta-large-Euclid-clarin-pl-poquad-1000\n",
      "  Generating with QA model: radlab/polish-qa-v2\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Processing retriever: sdadas/mmlw-retrieval-roberta-large-Euclid-clarin-pl-poquad-1000\n",
      "  Generating with QA model: radlab/polish-qa-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 20 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 30 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 40 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 50 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 60 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 70 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 80 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 90 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 100 questions for radlab/polish-qa-v2 on poquad\n",
      "  Generating with QA model: timpal0l/mdeberta-v3-base-squad2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 20 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 30 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 40 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 50 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 60 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 70 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 80 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 90 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 100 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "  Generating with Instruction model: ../../models/Bielik-11B-v2.2-Instruct-q4\n",
      "Processed 10 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/Mistral-7B-Instruct-v0.2-q4\n",
      "Processed 10 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/Mistral-7B-Instruct-v0.2-q4\n",
      "Processed 10 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 20 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 30 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 40 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 50 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 60 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 70 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 80 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 90 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 100 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/PLLuM-12-B-instruct-q4\n",
      "Processed 10 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 20 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 30 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 40 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 50 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 60 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 70 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 80 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 90 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 100 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/PLLuM-12-B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 10 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/PLLuM-8B-instruct-q4\n",
      "Processed 100 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/PLLuM-8B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processing retriever: basic_index-clarin-pl-poquad-500\n",
      "  Generating with QA model: radlab/polish-qa-v2\n",
      "Processed 10 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processing retriever: basic_index-clarin-pl-poquad-500\n",
      "  Generating with QA model: radlab/polish-qa-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 20 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 30 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 40 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 50 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 60 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 70 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 80 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 90 questions for radlab/polish-qa-v2 on poquad\n",
      "Processed 100 questions for radlab/polish-qa-v2 on poquad\n",
      "  Generating with QA model: timpal0l/mdeberta-v3-base-squad2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 20 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 30 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 40 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 50 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 60 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 70 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 80 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 90 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "Processed 100 questions for timpal0l/mdeberta-v3-base-squad2 on poquad\n",
      "  Generating with Instruction model: ../../models/Bielik-11B-v2.2-Instruct-q4\n",
      "Processed 10 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/Mistral-7B-Instruct-v0.2-q4\n",
      "Processed 10 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/Mistral-7B-Instruct-v0.2-q4\n",
      "Processed 10 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 20 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 30 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 40 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 50 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 60 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 70 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 80 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 90 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 100 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/PLLuM-12-B-instruct-q4\n",
      "Processed 10 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 20 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 30 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 40 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 50 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 60 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 70 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 80 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 90 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "Processed 100 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/PLLuM-12-B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/PLLuM-8B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/PLLuM-12-B-instruct-q4 on poquad\n",
      "  Generating with Instruction model: ../../models/PLLuM-8B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "\n",
      "=== Processing PoQuAD OpenAI dataset ===\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Processing retriever: text-embedding-3-large-Cosine-clarin-pl-poquad-2000\n",
      "  Generating with OpenAI model: gpt-4o-mini\n",
      "Processed 10 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 20 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 30 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 40 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 50 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 60 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 70 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 80 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 90 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "Processed 100 questions for ../../models/PLLuM-8B-instruct-q4 on poquad\n",
      "\n",
      "=== Processing PoQuAD OpenAI dataset ===\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Processing retriever: text-embedding-3-large-Cosine-clarin-pl-poquad-2000\n",
      "  Generating with OpenAI model: gpt-4o-mini\n",
      "Processed 10 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 20 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 30 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 40 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 50 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 60 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 70 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 80 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 90 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 100 questions for gpt-4o-mini on poquad_openai\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Processing retriever: text-embedding-3-large-Cosine-clarin-pl-poquad-500\n",
      "  Generating with OpenAI model: gpt-4o-mini\n",
      "Processed 10 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 20 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 30 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 40 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 10 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 20 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 30 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 40 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 50 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 60 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 70 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 80 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 90 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 100 questions for gpt-4o-mini on poquad_openai\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Processing retriever: text-embedding-3-large-Cosine-clarin-pl-poquad-500\n",
      "  Generating with OpenAI model: gpt-4o-mini\n",
      "Processed 10 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 20 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 30 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 40 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 50 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 60 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 70 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 80 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 90 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 100 questions for gpt-4o-mini on poquad_openai\n",
      "\n",
      "=== Processing PolQA dataset ===\n",
      "Processed 50 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 60 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 70 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 80 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 90 questions for gpt-4o-mini on poquad_openai\n",
      "Processed 100 questions for gpt-4o-mini on poquad_openai\n",
      "\n",
      "=== Processing PolQA dataset ===\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Processing retriever: morfologik_index-sdadas/mmlw-retrieval-roberta-large-Cosine-ipipan-polqa-1000-0.5-sdadas/polish-reranker-large-ranknet\n",
      "  Generating with QA model: radlab/polish-qa-v2\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Processing retriever: morfologik_index-sdadas/mmlw-retrieval-roberta-large-Cosine-ipipan-polqa-1000-0.5-sdadas/polish-reranker-large-ranknet\n",
      "  Generating with QA model: radlab/polish-qa-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 20 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 30 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 40 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 50 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 60 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 70 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 80 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 90 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 100 questions for radlab/polish-qa-v2 on polqa\n",
      "  Generating with QA model: timpal0l/mdeberta-v3-base-squad2\n",
      "Processed 80 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 90 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 100 questions for radlab/polish-qa-v2 on polqa\n",
      "  Generating with QA model: timpal0l/mdeberta-v3-base-squad2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 20 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 30 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 40 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 50 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 60 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 70 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 80 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 90 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 100 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "  Generating with Instruction model: ../../models/Bielik-11B-v2.2-Instruct-q4\n",
      "Processed 10 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 10 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/Mistral-7B-Instruct-v0.2-q4\n",
      "Processed 100 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/Mistral-7B-Instruct-v0.2-q4\n",
      "Processed 10 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 20 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 30 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 40 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 50 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 60 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 70 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 80 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 90 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 100 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/PLLuM-12-B-instruct-q4\n",
      "Processed 10 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 20 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 30 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 40 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 50 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 60 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 70 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 80 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 90 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 100 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/PLLuM-12-B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/PLLuM-8B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/PLLuM-8B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 10 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Cosine repository initialized\n",
      "Processing retriever: morfologik_index-sdadas/mmlw-retrieval-roberta-large-Cosine-ipipan-polqa-1000-0.75\n",
      "  Generating with QA model: radlab/polish-qa-v2\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Cosine repository initialized\n",
      "Processing retriever: morfologik_index-sdadas/mmlw-retrieval-roberta-large-Cosine-ipipan-polqa-1000-0.75\n",
      "  Generating with QA model: radlab/polish-qa-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 20 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 30 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 40 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 50 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 60 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 70 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 80 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 90 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 100 questions for radlab/polish-qa-v2 on polqa\n",
      "  Generating with QA model: timpal0l/mdeberta-v3-base-squad2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 20 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 30 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 40 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 50 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 60 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 70 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 80 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 90 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 100 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "  Generating with Instruction model: ../../models/Bielik-11B-v2.2-Instruct-q4\n",
      "Processed 10 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/Mistral-7B-Instruct-v0.2-q4\n",
      "Processed 10 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/Mistral-7B-Instruct-v0.2-q4\n",
      "Processed 10 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 20 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 30 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 40 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 50 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 60 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 70 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 80 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 90 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 100 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/PLLuM-12-B-instruct-q4\n",
      "Processed 10 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 20 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 30 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 40 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 50 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 60 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 70 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 80 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 90 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "Processed 100 questions for ../../models/Mistral-7B-Instruct-v0.2-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/PLLuM-12-B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/PLLuM-8B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/PLLuM-12-B-instruct-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/PLLuM-8B-instruct-q4\n",
      "Processed 10 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processing retriever: basic_index-ipipan-polqa-500\n",
      "  Generating with QA model: radlab/polish-qa-v2\n",
      "Processed 10 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/PLLuM-8B-instruct-q4 on polqa\n",
      "Processing retriever: basic_index-ipipan-polqa-500\n",
      "  Generating with QA model: radlab/polish-qa-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 20 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 30 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 40 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 50 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 60 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 70 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 40 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 50 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 60 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 70 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 80 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 90 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 100 questions for radlab/polish-qa-v2 on polqa\n",
      "  Generating with QA model: timpal0l/mdeberta-v3-base-squad2\n",
      "Processed 80 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 90 questions for radlab/polish-qa-v2 on polqa\n",
      "Processed 100 questions for radlab/polish-qa-v2 on polqa\n",
      "  Generating with QA model: timpal0l/mdeberta-v3-base-squad2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 20 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 30 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 40 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 50 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 60 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 70 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 80 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 90 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "Processed 100 questions for timpal0l/mdeberta-v3-base-squad2 on polqa\n",
      "  Generating with Instruction model: ../../models/Bielik-11B-v2.2-Instruct-q4\n",
      "Processed 10 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/Mistral-7B-Instruct-v0.2-q4\n",
      "Processed 10 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 20 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 30 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 40 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 50 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 60 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 70 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 80 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 90 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "Processed 100 questions for ../../models/Bielik-11B-v2.2-Instruct-q4 on polqa\n",
      "  Generating with Instruction model: ../../models/Mistral-7B-Instruct-v0.2-q4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inst_model_path \u001b[38;5;129;01min\u001b[39;00m INST_MODEL_PATHS:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Generating with Instruction model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     generator \u001b[38;5;241m=\u001b[39m \u001b[43mInstructionGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     metadata, header, results \u001b[38;5;241m=\u001b[39m generate_manual_evaluation_file(\n\u001b[1;32m     82\u001b[0m         retriever, retriever_name, generator, inst_model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     83\u001b[0m         polqa_dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolqa\u001b[39m\u001b[38;5;124m\"\u001b[39m, manual_eval_n\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     86\u001b[0m     filename \u001b[38;5;241m=\u001b[39m create_safe_filename(retriever_name, inst_model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolqa\u001b[39m\u001b[38;5;124m\"\u001b[39m, manual_eval_n)\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/src/notebooks/../generators/instruction_generator.py:13\u001b[0m, in \u001b[0;36mInstructionGenerator.__init__\u001b[0;34m(self, model_name, cache)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m, cache: Cache):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m---> 13\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/mlx_lm/utils.py:241\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path_or_hf_repo, tokenizer_config, model_config, adapter_path, lazy)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03mLoad the model and tokenizer from a given path or a huggingface repository.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    ValueError: If model class or args class are not found.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m model_path \u001b[38;5;241m=\u001b[39m get_model_path(path_or_hf_repo)\n\u001b[0;32m--> 241\u001b[0m model, config \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     model \u001b[38;5;241m=\u001b[39m load_adapters(model, adapter_path)\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/mlx_lm/utils.py:205\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_path, lazy, strict, model_config, get_model_classes)\u001b[0m\n\u001b[1;32m    202\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;28mlist\u001b[39m(weights\u001b[38;5;241m.\u001b[39mitems()), strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lazy:\n\u001b[0;32m--> 205\u001b[0m     \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, config\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate manual evaluation files for all combinations used in notebook 02\n",
    "print(\"Generating manual evaluation files for all combinations...\")\n",
    "\n",
    "# We'll use n=5 for manual evaluation as it includes more context\n",
    "manual_eval_n = 5\n",
    "\n",
    "# PoQuAD dataset combinations\n",
    "print(\"\\n=== Processing PoQuAD dataset ===\")\n",
    "for retriever_func in poquad_retriever_functions:\n",
    "    retriever, retriever_name = retriever_func()\n",
    "    print(f\"Processing retriever: {retriever_name}\")\n",
    "    \n",
    "    # QA Models\n",
    "    for qa_model_name in QA_MODEL_NAMES:\n",
    "        print(f\"  Generating with QA model: {qa_model_name}\")\n",
    "        generator = QuestionAnsweringGenerator(qa_model_name, cache)\n",
    "        \n",
    "        metadata, header, results = generate_manual_evaluation_file(\n",
    "            retriever, retriever_name, generator, qa_model_name, \"QA\", \n",
    "            poquad_dataset, \"poquad\", manual_eval_n\n",
    "        )\n",
    "        \n",
    "        filename = create_safe_filename(retriever_name, qa_model_name, \"QA\", \"poquad\", manual_eval_n)\n",
    "        save_manual_evaluation_file(metadata, header, results, f\"../../output/manual_eval/{filename}\")\n",
    "    \n",
    "    # Instruction Models\n",
    "    for inst_model_path in INST_MODEL_PATHS:\n",
    "        print(f\"  Generating with Instruction model: {inst_model_path}\")\n",
    "        generator = InstructionGenerator(inst_model_path, cache)\n",
    "        \n",
    "        metadata, header, results = generate_manual_evaluation_file(\n",
    "            retriever, retriever_name, generator, inst_model_path, \"INST\", \n",
    "            poquad_dataset, \"poquad\", manual_eval_n\n",
    "        )\n",
    "        \n",
    "        filename = create_safe_filename(retriever_name, inst_model_path, \"INST\", \"poquad\", manual_eval_n)\n",
    "        save_manual_evaluation_file(metadata, header, results, f\"../../output/manual_eval/{filename}\")\n",
    "\n",
    "# PoQuAD OpenAI combinations\n",
    "print(\"\\n=== Processing PoQuAD OpenAI dataset ===\")\n",
    "for retriever_func in poquad_openai_retriever_functions:\n",
    "    retriever, retriever_name = retriever_func()\n",
    "    print(f\"Processing retriever: {retriever_name}\")\n",
    "    \n",
    "    # OpenAI Generator\n",
    "    print(f\"  Generating with OpenAI model: gpt-4o-mini\")\n",
    "    generator = OpenAIGenerator(cache)\n",
    "    \n",
    "    metadata, header, results = generate_manual_evaluation_file(\n",
    "        retriever, retriever_name, generator, \"gpt-4o-mini\", \"INST\", \n",
    "        poquad_dataset, \"poquad_openai\", manual_eval_n\n",
    "    )\n",
    "    \n",
    "    filename = create_safe_filename(retriever_name, \"gpt-4o-mini\", \"INST\", \"poquad_openai\", manual_eval_n)\n",
    "    save_manual_evaluation_file(metadata, header, results, f\"../../output/manual_eval/{filename}\")\n",
    "\n",
    "# PolQA dataset combinations\n",
    "print(\"\\n=== Processing PolQA dataset ===\")\n",
    "for retriever_func in polqa_retriever_functions:\n",
    "    retriever, retriever_name = retriever_func()\n",
    "    print(f\"Processing retriever: {retriever_name}\")\n",
    "    \n",
    "    # QA Models\n",
    "    for qa_model_name in QA_MODEL_NAMES:\n",
    "        print(f\"  Generating with QA model: {qa_model_name}\")\n",
    "        generator = QuestionAnsweringGenerator(qa_model_name, cache)\n",
    "        \n",
    "        metadata, header, results = generate_manual_evaluation_file(\n",
    "            retriever, retriever_name, generator, qa_model_name, \"QA\", \n",
    "            polqa_dataset, \"polqa\", manual_eval_n\n",
    "        )\n",
    "        \n",
    "        filename = create_safe_filename(retriever_name, qa_model_name, \"QA\", \"polqa\", manual_eval_n)\n",
    "        save_manual_evaluation_file(metadata, header, results, f\"../../output/manual_eval/{filename}\")\n",
    "    \n",
    "    # Instruction Models\n",
    "    for inst_model_path in INST_MODEL_PATHS:\n",
    "        print(f\"  Generating with Instruction model: {inst_model_path}\")\n",
    "        generator = InstructionGenerator(inst_model_path, cache)\n",
    "        \n",
    "        metadata, header, results = generate_manual_evaluation_file(\n",
    "            retriever, retriever_name, generator, inst_model_path, \"INST\", \n",
    "            polqa_dataset, \"polqa\", manual_eval_n\n",
    "        )\n",
    "        \n",
    "        filename = create_safe_filename(retriever_name, inst_model_path, \"INST\", \"polqa\", manual_eval_n)\n",
    "        save_manual_evaluation_file(metadata, header, results, f\"../../output/manual_eval/{filename}\")\n",
    "\n",
    "# PolQA OpenAI combinations\n",
    "print(\"\\n=== Processing PolQA OpenAI dataset ===\")\n",
    "for retriever_func in polqa_openai_retriever_functions:\n",
    "    retriever, retriever_name = retriever_func()\n",
    "    print(f\"Processing retriever: {retriever_name}\")\n",
    "    \n",
    "    # OpenAI Generator\n",
    "    print(f\"  Generating with OpenAI model: gpt-4o-mini\")\n",
    "    generator = OpenAIGenerator(cache)\n",
    "    \n",
    "    metadata, header, results = generate_manual_evaluation_file(\n",
    "        retriever, retriever_name, generator, \"gpt-4o-mini\", \"INST\", \n",
    "        polqa_dataset, \"polqa_openai\", manual_eval_n\n",
    "    )\n",
    "    \n",
    "    filename = create_safe_filename(retriever_name, \"gpt-4o-mini\", \"INST\", \"polqa_openai\", manual_eval_n)\n",
    "    save_manual_evaluation_file(metadata, header, results, f\"../../output/manual_eval/{filename}\")\n",
    "\n",
    "print(\"\\n✅ All manual evaluation files generated successfully!\")\n",
    "print(\"Files saved in: ../../output/manual_eval/\")\n",
    "print(\"Each file contains:\")\n",
    "print(\"- Metadata header with retriever, generator, type, dataset, and n info\")\n",
    "print(\"- CSV with columns: question, question_id, hasCorrectPassages, answer, correct_answer, result\")\n",
    "print(\"- Empty 'result' column for your manual evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
