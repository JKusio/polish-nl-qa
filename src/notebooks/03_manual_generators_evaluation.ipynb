{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a9009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533bd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from qdrant_client import QdrantClient\n",
    "from cache.cache import Cache\n",
    "\n",
    "\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "es_client = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"],\n",
    ")\n",
    "cache = Cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5fde4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.names import (\n",
    "    OPENAI_EMBEDDING_MODEL_NAMES,\n",
    "    PASSAGE_PREFIX_MAP,\n",
    "    QUERY_PREFIX_MAP,\n",
    ")\n",
    "from repository.es_repository import ESRepository\n",
    "from repository.qdrant_openai_repository import QdrantOpenAIRepository\n",
    "from repository.qdrant_repository import QdrantRepository\n",
    "from qdrant_client.models import Distance\n",
    "\n",
    "from rerankers.hf_reranker import HFReranker\n",
    "from retrievers.es_retriever import ESRetriever\n",
    "from retrievers.hybrid_retriever import HybridRetriever\n",
    "from retrievers.qdrant_retriever import QdrantRetriever\n",
    "from retrievers.retriever import Retriever\n",
    "\n",
    "\n",
    "def get_best_poquad_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"clarin-pl-poquad-100000\"\n",
    "    es_index = \"morfologik_index\"\n",
    "    qdrant_model = \"intfloat/multilingual-e5-large\"\n",
    "    reranker_model = \"sdadas/polish-reranker-large-ranknet\"\n",
    "    alpha = 0.5\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "    passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "    query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "    qdrant_repository = QdrantRepository.get_repository(\n",
    "        qdrant_client,\n",
    "        qdrant_model,\n",
    "        Distance.COSINE,\n",
    "        cache,\n",
    "        passage_prefix,\n",
    "        query_prefix,\n",
    "    )\n",
    "    reranker = HFReranker(reranker_model, cache)\n",
    "\n",
    "    retriever = HybridRetriever(\n",
    "        es_repository, qdrant_repository, dataset_key, alpha, reranker\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"morfologik_index-intfloat/multilingual-e5-large-Cosine-clarin-pl-poquad-100000-0.5-sdadas/polish-reranker-large-ranknet\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_50p_poquad_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"clarin-pl-poquad-1000\"\n",
    "    qdrant_model = \"sdadas/mmlw-retrieval-roberta-large\"\n",
    "\n",
    "    passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "    query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "    qdrant_repository = QdrantRepository.get_repository(\n",
    "        qdrant_client,\n",
    "        qdrant_model,\n",
    "        Distance.EUCLID,\n",
    "        cache,\n",
    "        passage_prefix,\n",
    "        query_prefix,\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(qdrant_repository, dataset_key)\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"sdadas/mmlw-retrieval-roberta-large-Euclid-clarin-pl-poquad-1000\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_worst_poquad_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"clarin-pl-poquad-500\"\n",
    "    es_index = \"basic_index\"\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "\n",
    "    retriever = ESRetriever(es_repository, dataset_key)\n",
    "\n",
    "    return (retriever, \"basic_index-clarin-pl-poquad-500\")\n",
    "\n",
    "\n",
    "def get_best_poquad_openai_retriever() -> tuple[Retriever, str]:\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.COSINE, cache\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(repository, \"clarin-pl-poquad-2000\")\n",
    "\n",
    "    return (retriever, \"text-embedding-3-large-Cosine-clarin-pl-poquad-2000\")\n",
    "\n",
    "\n",
    "def get_worst_poquad_openai_retriever() -> tuple[Retriever, str]:\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.COSINE, cache\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(repository, \"clarin-pl-poquad-500\")\n",
    "\n",
    "    return (retriever, \"text-embedding-3-large-Cosine-clarin-pl-poquad-500\")\n",
    "\n",
    "\n",
    "def get_best_polqa_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"ipipan-polqa-1000\"\n",
    "    es_index = \"morfologik_index\"\n",
    "    qdrant_model = \"sdadas/mmlw-retrieval-roberta-large\"\n",
    "    reranker_model = \"sdadas/polish-reranker-large-ranknet\"\n",
    "    alpha = 0.75\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "    passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "    query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "    qdrant_repository = QdrantRepository.get_repository(\n",
    "        qdrant_client,\n",
    "        qdrant_model,\n",
    "        Distance.COSINE,\n",
    "        cache,\n",
    "        passage_prefix,\n",
    "        query_prefix,\n",
    "    )\n",
    "    reranker = HFReranker(reranker_model, cache)\n",
    "\n",
    "    retriever = HybridRetriever(\n",
    "        es_repository, qdrant_repository, dataset_key, alpha, reranker\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"morfologik_index-sdadas/mmlw-retrieval-roberta-large-Cosine-ipipan-polqa-1000-0.5-sdadas/polish-reranker-large-ranknet\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_50p_polqa_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"ipipan-polqa-1000\"\n",
    "    es_index = \"morfologik_index\"\n",
    "    qdrant_model = \"sdadas/mmlw-retrieval-roberta-large\"\n",
    "    alpha = 0.75\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "    passage_prefix = PASSAGE_PREFIX_MAP[qdrant_model]\n",
    "    query_prefix = QUERY_PREFIX_MAP[qdrant_model]\n",
    "    qdrant_repository = QdrantRepository.get_repository(\n",
    "        qdrant_client,\n",
    "        qdrant_model,\n",
    "        Distance.COSINE,\n",
    "        cache,\n",
    "        passage_prefix,\n",
    "        query_prefix,\n",
    "    )\n",
    "\n",
    "    retriever = HybridRetriever(es_repository, qdrant_repository, dataset_key, alpha)\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"morfologik_index-sdadas/mmlw-retrieval-roberta-large-Cosine-ipipan-polqa-1000-0.75\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_worst_polqa_retriever() -> tuple[Retriever, str]:\n",
    "    dataset_key = \"ipipan-polqa-500\"\n",
    "    es_index = \"basic_index\"\n",
    "\n",
    "    es_repository = ESRepository(es_client, es_index, cache)\n",
    "\n",
    "    retriever = ESRetriever(es_repository, dataset_key)\n",
    "\n",
    "    return (\n",
    "        retriever,\n",
    "        \"basic_index-ipipan-polqa-500\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_best_polqa_openai_retriever() -> tuple[Retriever, str]:\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.EUCLID, cache\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(repository, \"ipipan-polqa-2000\")\n",
    "\n",
    "    return (retriever, \"text-embedding-3-large-Euclid-ipipan-polqa-2000\")\n",
    "\n",
    "\n",
    "def get_worst_polqa_openai_retriever() -> tuple[Retriever, str]:\n",
    "    repository = QdrantOpenAIRepository.get_repository(\n",
    "        qdrant_client, OPENAI_EMBEDDING_MODEL_NAMES[0], Distance.COSINE, cache\n",
    "    )\n",
    "\n",
    "    retriever = QdrantRetriever(repository, \"ipipan-polqa-500\")\n",
    "\n",
    "    return (retriever, \"text-embedding-3-large-Cosine-ipipan-polqa-500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7929b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "poquad_retriever_functions = [\n",
    "    get_best_poquad_retriever,\n",
    "    get_50p_poquad_retriever,\n",
    "    get_worst_poquad_retriever,\n",
    "]\n",
    "\n",
    "poquad_openai_retriever_functions = [\n",
    "    get_best_poquad_openai_retriever,\n",
    "    get_worst_poquad_openai_retriever,\n",
    "]\n",
    "\n",
    "polqa_retriever_functions = [\n",
    "    get_best_polqa_retriever,\n",
    "    get_50p_polqa_retriever,\n",
    "    get_worst_polqa_retriever,\n",
    "]\n",
    "\n",
    "polqa_openai_retriever_functions = [\n",
    "    get_best_polqa_openai_retriever,\n",
    "    get_worst_polqa_openai_retriever,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbbc8957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.names import DATASET_SEED\n",
    "from dataset.polqa_dataset_getter import PolqaDatasetGetter\n",
    "from dataset.poquad_dataset_getter import PoquadDatasetGetter\n",
    "\n",
    "\n",
    "poquad_dataset_getter = PoquadDatasetGetter()\n",
    "polqa_dataset_getter = PolqaDatasetGetter()\n",
    "\n",
    "poquad_dataset = poquad_dataset_getter.get_random_n_test(500, DATASET_SEED)[:100]\n",
    "polqa_dataset = polqa_dataset_getter.get_random_n_test(500, DATASET_SEED)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2666c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33deb110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrievers_top_n(retriever_functions, dataset, ns):\n",
    "    results = {}\n",
    "    for get_retriever in retriever_functions:\n",
    "        retriever, retriever_name = get_retriever()\n",
    "        retriever_results = {}\n",
    "        for n in ns:\n",
    "            hits = []\n",
    "            for item in dataset:\n",
    "                question = item.question \n",
    "                correct_passage_id = item.passage_id \n",
    "                retriever_result = retriever.get_relevant_passages(question)\n",
    "                passages = [passage for (passage, _) in retriever_result.passages]\n",
    "                top_n_passages = passages[:n]\n",
    "                retrieved_ids = [passage.id for passage in top_n_passages]\n",
    "                hits.append(correct_passage_id in retrieved_ids)\n",
    "            retriever_results[n] = hits\n",
    "        results[retriever_name] = retriever_results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "825a7d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Cosine repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Euclid repository initialized\n",
      "Vectorizer with model text-embedding-3-large initialized\n",
      "Qdrant openai collection text-embedding-3-large-Cosine repository initialized\n"
     ]
    }
   ],
   "source": [
    "retriever_evaluation_results = {\n",
    "    \"poquad\": evaluate_retrievers_top_n(poquad_retriever_functions, poquad_dataset, ns),\n",
    "    \"poquad_openai\": evaluate_retrievers_top_n(poquad_openai_retriever_functions, poquad_dataset, ns),\n",
    "    \"polqa\": evaluate_retrievers_top_n(polqa_retriever_functions, polqa_dataset, ns),\n",
    "    \"polqa_openai\": evaluate_retrievers_top_n(polqa_openai_retriever_functions, polqa_dataset, ns),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34586353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results as csv\n",
    "import csv\n",
    "\n",
    "\n",
    "def save_retriever_scores_to_csv(results, filename):\n",
    "    with open(filename, mode=\"w\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"dataset\", \"retriever\", \"n\", \"hit_rate\"])\n",
    "        \n",
    "        for dataset_name, dataset_results in results.items():\n",
    "            for retriever_name, retriever_results in dataset_results.items():\n",
    "                for n, hits in retriever_results.items():\n",
    "                    hit_rate = sum(hits) / len(hits)\n",
    "                    writer.writerow(\n",
    "                        [\n",
    "                            dataset_name,\n",
    "                            retriever_name,\n",
    "                            str(n),\n",
    "                            str(hit_rate).replace(\".\", \",\"),\n",
    "                        ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f3dc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_retriever_scores_to_csv(retriever_evaluation_results, \"../../output/retriever_evaluation_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c02b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_detailed_question_results(retriever_functions, dataset, ns, dataset_name):\n",
    "    \"\"\"Generate detailed results for each question showing which retrievers found the correct passage\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Create header with question info and all retriever-n combinations\n",
    "    header = [\"question_id\", \"question_text\", \"correct_passage_id\"]\n",
    "    retriever_columns = []\n",
    "    \n",
    "    for get_retriever in retriever_functions:\n",
    "        _, retriever_name = get_retriever()\n",
    "        for n in ns:\n",
    "            column_name = f\"{retriever_name}_n{n}\"\n",
    "            retriever_columns.append(column_name)\n",
    "            header.append(column_name)\n",
    "    \n",
    "    results.append(header)\n",
    "    \n",
    "    # Pre-load all retrievers once\n",
    "    retrievers = []\n",
    "    for get_retriever in retriever_functions:\n",
    "        print(f\"Loading retriever...\")\n",
    "        retriever, retriever_name = get_retriever()\n",
    "        retrievers.append((retriever, retriever_name))\n",
    "        print(f\"Loaded: {retriever_name}\")\n",
    "    \n",
    "    # Process each question\n",
    "    for i, item in enumerate(dataset):\n",
    "        question_id = f\"{dataset_name}_q{i+1}\"\n",
    "        question_text = item.question\n",
    "        correct_passage_id = item.passage_id\n",
    "        \n",
    "        row = [question_id, question_text, correct_passage_id]\n",
    "        \n",
    "        # Test each pre-loaded retriever configuration\n",
    "        for retriever, retriever_name in retrievers:\n",
    "            # Get retrieval results once for this question\n",
    "            retriever_result = retriever.get_relevant_passages(question_text)\n",
    "            passages = [passage for (passage, _) in retriever_result.passages]\n",
    "            \n",
    "            for n in ns:\n",
    "                top_n_passages = passages[:n]\n",
    "                retrieved_ids = [passage.id for passage in top_n_passages]\n",
    "                found = correct_passage_id in retrieved_ids\n",
    "                row.append(str(found).upper())\n",
    "        \n",
    "        results.append(row)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1} questions for {dataset_name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def save_detailed_results_to_csv(results, filename):\n",
    "    \"\"\"Save detailed question-level results to CSV\"\"\"\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in results:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e911a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating detailed question-level results...\n",
      "Processing PoQuAD dataset...\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model sdadas/mmlw-retrieval-roberta-large initialized\n",
      "Qdrant collection sdadas-mmlw-retrieval-roberta-large-Euclid repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model intfloat/multilingual-e5-large initialized\n",
      "Qdrant collection intfloat-multilingual-e5-large-Cosine repository initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n",
      "Vectorizer with model sdadas/polish-reranker-large-ranknet initialized\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# PoQuAD dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing PoQuAD dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m poquad_detailed \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_detailed_question_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoquad_retriever_functions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoquad_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoquad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m save_detailed_results_to_csv(poquad_detailed, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../output/poquad_detailed_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# PoQuAD OpenAI dataset  \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m, in \u001b[0;36mgenerate_detailed_question_results\u001b[0;34m(retriever_functions, dataset, ns, dataset_name)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Test each retriever configuration\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m get_retriever \u001b[38;5;129;01min\u001b[39;00m retriever_functions:\n\u001b[0;32m---> 28\u001b[0m     retriever, retriever_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Get retrieval results once for this question\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     retriever_result \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mget_relevant_passages(question_text)\n",
      "Cell \u001b[0;32mIn[2], line 54\u001b[0m, in \u001b[0;36mget_50p_poquad_retriever\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m passage_prefix \u001b[38;5;241m=\u001b[39m PASSAGE_PREFIX_MAP[qdrant_model]\n\u001b[1;32m     53\u001b[0m query_prefix \u001b[38;5;241m=\u001b[39m QUERY_PREFIX_MAP[qdrant_model]\n\u001b[0;32m---> 54\u001b[0m qdrant_repository \u001b[38;5;241m=\u001b[39m \u001b[43mQdrantRepository\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_repository\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqdrant_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqdrant_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDistance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEUCLID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassage_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m retriever \u001b[38;5;241m=\u001b[39m QdrantRetriever(qdrant_repository, dataset_key)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     66\u001b[0m     retriever,\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdadas/mmlw-retrieval-roberta-large-Euclid-clarin-pl-poquad-1000\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/src/notebooks/../repository/qdrant_repository.py:188\u001b[0m, in \u001b[0;36mQdrantRepository.get_repository\u001b[0;34m(client, model_name, distance, cache, passage_prefix, query_prefix)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_repository\u001b[39m(\n\u001b[1;32m    180\u001b[0m     client: QdrantClient,\n\u001b[1;32m    181\u001b[0m     model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     query_prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    186\u001b[0m ):\n\u001b[1;32m    187\u001b[0m     collection_name \u001b[38;5;241m=\u001b[39m get_qdrant_collection_name(model_name, distance)\n\u001b[0;32m--> 188\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mHFVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QdrantRepository(\n\u001b[1;32m    191\u001b[0m         client,\n\u001b[1;32m    192\u001b[0m         collection_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m         query_prefix,\n\u001b[1;32m    199\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/src/notebooks/../vectorizer/hf_vectorizer.py:15\u001b[0m, in \u001b[0;36mHFVectorizer.__init__\u001b[0;34m(self, model_name, cache)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmax_seq_length\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m cache\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:348\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (5 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Workspace/polish-nl-qa/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate detailed results for each dataset\n",
    "print(\"Generating detailed question-level results...\")\n",
    "\n",
    "# PoQuAD dataset\n",
    "print(\"Processing PoQuAD dataset...\")\n",
    "poquad_detailed = generate_detailed_question_results(\n",
    "    poquad_retriever_functions, poquad_dataset, ns, \"poquad\"\n",
    ")\n",
    "save_detailed_results_to_csv(poquad_detailed, \"../../output/poquad_detailed_results.csv\")\n",
    "\n",
    "# PoQuAD OpenAI dataset  \n",
    "print(\"Processing PoQuAD OpenAI dataset...\")\n",
    "poquad_openai_detailed = generate_detailed_question_results(\n",
    "    poquad_openai_retriever_functions, poquad_dataset, ns, \"poquad_openai\"\n",
    ")\n",
    "save_detailed_results_to_csv(poquad_openai_detailed, \"../../output/poquad_openai_detailed_results.csv\")\n",
    "\n",
    "# PolQA dataset\n",
    "print(\"Processing PolQA dataset...\")\n",
    "polqa_detailed = generate_detailed_question_results(\n",
    "    polqa_retriever_functions, polqa_dataset, ns, \"polqa\"\n",
    ")\n",
    "save_detailed_results_to_csv(polqa_detailed, \"../../output/polqa_detailed_results.csv\")\n",
    "\n",
    "# PolQA OpenAI dataset\n",
    "print(\"Processing PolQA OpenAI dataset...\")\n",
    "polqa_openai_detailed = generate_detailed_question_results(\n",
    "    polqa_openai_retriever_functions, polqa_dataset, ns, \"polqa_openai\"\n",
    ")\n",
    "save_detailed_results_to_csv(polqa_openai_detailed, \"../../output/polqa_openai_detailed_results.csv\")\n",
    "\n",
    "print(\"All detailed results saved to CSV files!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
